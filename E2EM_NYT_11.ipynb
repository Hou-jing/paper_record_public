{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hou-jing/paper_record_public/blob/main/E2EM_NYT_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4ioLtbgcKhN",
        "outputId": "1a81421f-14fe-4137-a5fd-d5bf87da5197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 4.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 30.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 56.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Au7jv5Bhb_5_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re, os, json\n",
        "from random import choice\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BertModel,BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRqvDP6LcWUF"
      },
      "outputs": [],
      "source": [
        "model_name='bert-base-cased'\n",
        "BERT_MAX_LEN = 128\n",
        "RANDOM_SEED = 2019"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6IIevofcORX"
      },
      "outputs": [],
      "source": [
        "def to_tuple(sent):\n",
        "    triple_list = []\n",
        "    for triple in sent['triple_list']:\n",
        "        triple_list.append(tuple(triple))\n",
        "    sent['triple_list'] = triple_list\n",
        "\n",
        "def seq_padding(batch, padding=0):\n",
        "    # length_batch = [len(seq) for seq in batch]\n",
        "    # max_length = max(length_batch)\n",
        "    max_length=BERT_MAX_LEN\n",
        "\n",
        "    return np.array([\n",
        "        np.concatenate([seq, [padding] * (max_length - len(seq))]) if len(seq) < max_length else seq for seq in batch\n",
        "    ])\n",
        "\n",
        "def load_data(test_path, rel_dict_path):\n",
        "    test_data = json.load(open(test_path))\n",
        "    id2rel, rel2id = json.load(open(rel_dict_path))\n",
        "\n",
        "    id2rel = {int(i): j for i, j in id2rel.items()}\n",
        "    num_rels = len(id2rel)\n",
        "\n",
        "    for sent in test_data:\n",
        "        to_tuple(sent)\n",
        "\n",
        "    print(\"test_data len:\", len(test_data))\n",
        "\n",
        "    return test_data, id2rel, rel2id, num_rels\n",
        "\n",
        "def find_head_idx(source, target):\n",
        "    target_len = len(target)\n",
        "    for i in range(len(source)):\n",
        "        if source[i: i + target_len] == target:\n",
        "            return i\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv1eR4xk4dK9"
      },
      "source": [
        "### preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj28jQJwcZmV"
      },
      "outputs": [],
      "source": [
        "class data_generator:\n",
        "    def __init__(self, data, tokenizer, rel2id, num_rels, maxlen):\n",
        "        self.data = data\n",
        "        self.batch_size = len(self.data)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.maxlen = maxlen\n",
        "        self.rel2id = rel2id\n",
        "        self.num_rels = num_rels\n",
        "    def __len__(self):\n",
        "        return self.batch_size\n",
        "    def generator(self):\n",
        "        # while True:\n",
        "            idxs = list(range(len(self.data)))\n",
        "            # np.random.seed(RANDOM_SEED)\n",
        "            # np.random.shuffle(idxs)\n",
        "            tokens_batch, segments_batch, sub_heads_batch, sub_tails_batch, sub_head_batch, sub_tail_batch, obj_heads_batch, obj_tails_batch = [], [], [], [], [], [], [], []\n",
        "            for idx in idxs:\n",
        "                line = self.data[idx]\n",
        "                text = ' '.join(line['text'].split()[:self.maxlen])\n",
        "                tokens = self.tokenizer.tokenize(text)\n",
        "                if len(tokens) > BERT_MAX_LEN:\n",
        "                    tokens = tokens[:BERT_MAX_LEN]\n",
        "\n",
        "\n",
        "                s2ro_map = {}\n",
        "                for triple in line['triple_list']:\n",
        "                    triple = (self.tokenizer.tokenize(triple[0]), triple[1], self.tokenizer.tokenize(triple[2]))\n",
        "                    sub_head_idx = find_head_idx(tokens, triple[0])\n",
        "                    obj_head_idx = find_head_idx(tokens, triple[2])\n",
        "                    if sub_head_idx != 0 and obj_head_idx != 0:\n",
        "                        sub = (sub_head_idx, sub_head_idx + len(triple[0]) - 1)\n",
        "                        if sub not in s2ro_map:\n",
        "                            s2ro_map[sub] = []\n",
        "                        s2ro_map[sub].append((obj_head_idx,##subject to relation object\n",
        "                                           obj_head_idx + len(triple[2]) - 1,#obj_tail_idx\n",
        "                                           self.rel2id[triple[1]]))#rel\n",
        "                text_len = BERT_MAX_LEN\n",
        "                inputs=self.tokenizer(text,return_tensors='pt',add_special_tokens=False,truncation=True,padding=True,max_length=BERT_MAX_LEN)\n",
        "                token_ids, segment_ids=inputs['input_ids'],inputs['attention_mask']\n",
        "                pad_len=BERT_MAX_LEN-token_ids.shape[1]\n",
        "                pad_seq=torch.zeros(1,pad_len)\n",
        "                token_ids=torch.cat((token_ids,pad_seq),dim=1)\n",
        "                segment_ids=torch.cat((segment_ids,pad_seq),dim=1)\n",
        "                    # if inputs.shape[1]<BERT_MAX_LEN:\n",
        "                    #     inputs=inputs\n",
        "                    #     [seq, [padding] * (max_length - len(seq))]\n",
        "\n",
        "                    # if len(token_ids) > text_len:\n",
        "                    #     token_ids = token_ids[:text_len]\n",
        "                    #     segment_ids = segment_ids[:text_len]\n",
        "                tokens_batch.append(token_ids)\n",
        "                segments_batch.append(segment_ids)\n",
        "                sub_heads, sub_tails=torch.zeros(text_len),torch.zeros(text_len)\n",
        "                obj_heads, obj_tails = torch.zeros((text_len, self.num_rels)), torch.zeros((text_len, self.num_rels))\n",
        "                if s2ro_map:\n",
        "                    # token_ids, segment_ids = self.tokenizer.encode(text)\n",
        "                    \n",
        "                    \n",
        "                    for s in s2ro_map:\n",
        "                        sub_heads[s[0]] = 1\n",
        "                        sub_tails[s[1]] = 1\n",
        "                    # sub_head, sub_tail = choice(list(s2ro_map.keys()))\n",
        "\n",
        "                    \n",
        "                    sub=list(s2ro_map.keys())\n",
        "                    for sub_head,sub_tail in sub:\n",
        "                        for ro in s2ro_map.get((sub_head, sub_tail), []):\n",
        "                            obj_heads[ro[0]][ro[2]] = 1\n",
        "                            obj_tails[ro[1]][ro[2]] = 1\n",
        "                        # sub_head_batch.append([sub_head])\n",
        "                        # sub_tail_batch.append([sub_tail])\n",
        "                    # print(sub_heads,sub_tails,torch.where(obj_heads==1),torch.where(obj_tails)==1)\n",
        "                sub_heads_batch.append(sub_heads)\n",
        "                sub_tails_batch.append(sub_tails)\n",
        "                obj_heads_batch.append(obj_heads)\n",
        "                obj_tails_batch.append(obj_tails)\n",
        "            return tokens_batch, segments_batch, sub_heads_batch, sub_tails_batch, sub_head_batch, sub_tail_batch, obj_heads_batch, obj_tails_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK8ExhWExadZ"
      },
      "source": [
        "### download___NYT "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf3Jmrwzd7N1",
        "outputId": "2159b682-ab04-4cb4-aab9-4c9a575de87a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-12 13:16:35--  https://www.dropbox.com/s/u5u173tlze2er6z/Desktop.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/u5u173tlze2er6z/Desktop.zip [following]\n",
            "--2022-03-12 13:16:36--  https://www.dropbox.com/s/raw/u5u173tlze2er6z/Desktop.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce43f6a6bd473a44688ccfcaec4.dl.dropboxusercontent.com/cd/0/inline/BhW3ZNAKG1rjwt1GR0mNhxqM0ultVaDWZ7XRU-OYSB5Kd_LKBDgLZRFlJhvW1xcD9ponnIZpNMhH1sUMjOznYuydDG70nnhAq5wdy14h1-mgSdyztciCESmkAH_1Ax1AM5IviPIuLZCQkIJ_GbdeNOlBVu-c445em0ubWWe00nBtDw/file# [following]\n",
            "--2022-03-12 13:16:36--  https://uce43f6a6bd473a44688ccfcaec4.dl.dropboxusercontent.com/cd/0/inline/BhW3ZNAKG1rjwt1GR0mNhxqM0ultVaDWZ7XRU-OYSB5Kd_LKBDgLZRFlJhvW1xcD9ponnIZpNMhH1sUMjOznYuydDG70nnhAq5wdy14h1-mgSdyztciCESmkAH_1Ax1AM5IviPIuLZCQkIJ_GbdeNOlBVu-c445em0ubWWe00nBtDw/file\n",
            "Resolving uce43f6a6bd473a44688ccfcaec4.dl.dropboxusercontent.com (uce43f6a6bd473a44688ccfcaec4.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uce43f6a6bd473a44688ccfcaec4.dl.dropboxusercontent.com (uce43f6a6bd473a44688ccfcaec4.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BhULVWTwe3aOnwgeu_m2QTs7Fc9qUmwgYimPuNaPrBFUhwxKE8yTkyvvJBAyOhKQZUtzwssaVXktKNebf-bei1M_PPajOaZchHiqgy_7yL-RC1qN5xkMlOhBmb1LCBYX-XLVfl6f5gdZP4cIp7Jawzd7nLr5HeD-BOiKuVSqX99ryyLOcWUV2a5YisGShdL5-GIkrXuy5RUhy55vmOfBp-InwXEQqZPV4bHjnjCufa35a4oCyAAdif7eHQUFw-goLwWrKpsP82Txi0tZ8wQsb3nAmX5yILnK7KvFr533jFxk6BPyYWowsFZ-GigRmpDWK09uIYY2wyL12D-Nh5fXA47A_krGswAAleKiQKsLAvKvTcPQk2sebMhLFdbOvthJ2XvYVbFdpdTHn5uaVBAEbREQyMuRbV1XcyShOGjOkQjsMA/file [following]\n",
            "--2022-03-12 13:16:36--  https://uce43f6a6bd473a44688ccfcaec4.dl.dropboxusercontent.com/cd/0/inline2/BhULVWTwe3aOnwgeu_m2QTs7Fc9qUmwgYimPuNaPrBFUhwxKE8yTkyvvJBAyOhKQZUtzwssaVXktKNebf-bei1M_PPajOaZchHiqgy_7yL-RC1qN5xkMlOhBmb1LCBYX-XLVfl6f5gdZP4cIp7Jawzd7nLr5HeD-BOiKuVSqX99ryyLOcWUV2a5YisGShdL5-GIkrXuy5RUhy55vmOfBp-InwXEQqZPV4bHjnjCufa35a4oCyAAdif7eHQUFw-goLwWrKpsP82Txi0tZ8wQsb3nAmX5yILnK7KvFr533jFxk6BPyYWowsFZ-GigRmpDWK09uIYY2wyL12D-Nh5fXA47A_krGswAAleKiQKsLAvKvTcPQk2sebMhLFdbOvthJ2XvYVbFdpdTHn5uaVBAEbREQyMuRbV1XcyShOGjOkQjsMA/file\n",
            "Reusing existing connection to uce43f6a6bd473a44688ccfcaec4.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6651018 (6.3M) [application/zip]\n",
            "Saving to: ‘desktop.zip’\n",
            "\n",
            "desktop.zip         100%[===================>]   6.34M  25.4MB/s    in 0.2s    \n",
            "\n",
            "2022-03-12 13:16:37 (25.4 MB/s) - ‘desktop.zip’ saved [6651018/6651018]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Dropbox\n",
        "!wget https://www.dropbox.com/s/u5u173tlze2er6z/Desktop.zip?dl=0 -O desktop.zip\n",
        "\n",
        "# Unzip the dataset.\n",
        "# This may take some time.\n",
        "!unzip -q desktop.zip\n",
        "# !gdown--https://www.dropbox.com/s/u5u173tlze2er6z/Desktop.zip?dl=0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBIa--2vtTIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc05166c-0622-4ab3-f259-c6e40fb6bf8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-12 13:16:38--  https://www.dropbox.com/s/fcovcanizu6me70/dev_triples.json?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/fcovcanizu6me70/dev_triples.json [following]\n",
            "--2022-03-12 13:16:38--  https://www.dropbox.com/s/raw/fcovcanizu6me70/dev_triples.json\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucedc6a70f9755354fb670722770.dl.dropboxusercontent.com/cd/0/inline/BhUQX64KVaTbqJcbs8l89lKWOfKb1BFhMd0Yb_P9MX5tSicjwpquQ9MFsbFy9wu25f5OHQu69JQ0j-04HAlTzdGKVn3eHe0PybCHTN-sVkOWvkXgRhBKJIdotCl78y6rY0N8noMFZSk0vg6sLD40W1O3c4wML-uYvVGz4ovRw5fNGg/file# [following]\n",
            "--2022-03-12 13:16:38--  https://ucedc6a70f9755354fb670722770.dl.dropboxusercontent.com/cd/0/inline/BhUQX64KVaTbqJcbs8l89lKWOfKb1BFhMd0Yb_P9MX5tSicjwpquQ9MFsbFy9wu25f5OHQu69JQ0j-04HAlTzdGKVn3eHe0PybCHTN-sVkOWvkXgRhBKJIdotCl78y6rY0N8noMFZSk0vg6sLD40W1O3c4wML-uYvVGz4ovRw5fNGg/file\n",
            "Resolving ucedc6a70f9755354fb670722770.dl.dropboxusercontent.com (ucedc6a70f9755354fb670722770.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucedc6a70f9755354fb670722770.dl.dropboxusercontent.com (ucedc6a70f9755354fb670722770.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2396319 (2.3M) [text/plain]\n",
            "Saving to: ‘dev_triples.json’\n",
            "\n",
            "dev_triples.json    100%[===================>]   2.29M  14.5MB/s    in 0.2s    \n",
            "\n",
            "2022-03-12 13:16:39 (14.5 MB/s) - ‘dev_triples.json’ saved [2396319/2396319]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.dropbox.com/s/fcovcanizu6me70/dev_triples.json?dl=0  -O dev_triples.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50wYt-3ze6rA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f29f63-903d-4e31-a826-7136f50fd24a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-12 13:16:39--  https://www.dropbox.com/s/vkz83jfc9ekx1i5/rel2id.json?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/vkz83jfc9ekx1i5/rel2id.json [following]\n",
            "--2022-03-12 13:16:39--  https://www.dropbox.com/s/raw/vkz83jfc9ekx1i5/rel2id.json\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc194fc3c3a022ed61492d857cbd.dl.dropboxusercontent.com/cd/0/inline/BhXglNkX29AOoguRvmJH_aOdZiqGqTOw8MCiA-YPJJImlmV6N-n58UDATTtTfxFbz4e4vTD8uSjK5Yesayhv56h3Cns8qwFgZbD678YpCXNLYNme2K5nxyrpd7UKXYk_4A0r8YvXkYfr5utP4mf582T2Kem23DWNw0P4VlS4zt6Z9w/file# [following]\n",
            "--2022-03-12 13:16:39--  https://uc194fc3c3a022ed61492d857cbd.dl.dropboxusercontent.com/cd/0/inline/BhXglNkX29AOoguRvmJH_aOdZiqGqTOw8MCiA-YPJJImlmV6N-n58UDATTtTfxFbz4e4vTD8uSjK5Yesayhv56h3Cns8qwFgZbD678YpCXNLYNme2K5nxyrpd7UKXYk_4A0r8YvXkYfr5utP4mf582T2Kem23DWNw0P4VlS4zt6Z9w/file\n",
            "Resolving uc194fc3c3a022ed61492d857cbd.dl.dropboxusercontent.com (uc194fc3c3a022ed61492d857cbd.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc194fc3c3a022ed61492d857cbd.dl.dropboxusercontent.com (uc194fc3c3a022ed61492d857cbd.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2288 (2.2K) [text/plain]\n",
            "Saving to: ‘rel2id.json’\n",
            "\n",
            "rel2id.json         100%[===================>]   2.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-03-12 13:16:40 (325 MB/s) - ‘rel2id.json’ saved [2288/2288]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.dropbox.com/s/vkz83jfc9ekx1i5/rel2id.json?dl=0  -O rel2id.json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NYT_11_download"
      ],
      "metadata": {
        "id": "tGRk3Z2DR0Da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/kj5f1vcomr21xyv/NYT11.zip?dl=0  -O NYT11.zip \n",
        "!unzip -q NYT11.zip"
      ],
      "metadata": {
        "id": "IvDhLA8YR3gc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5b26f7f-1924-48c0-e969-33220799cae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-12 13:16:40--  https://www.dropbox.com/s/kj5f1vcomr21xyv/NYT11.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/kj5f1vcomr21xyv/NYT11.zip [following]\n",
            "--2022-03-12 13:16:40--  https://www.dropbox.com/s/raw/kj5f1vcomr21xyv/NYT11.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc67352ba0fb49de598d86077b6c.dl.dropboxusercontent.com/cd/0/inline/BhXO1eiiIJpaZcHwAKnJtDR_3hcJHj653piyLB3Qy6U5yM5TR4wUpxV0UxxBKy8H2cRDmgc22E727ZKEK4fPmsd3U7k5dCshLUkNqNxojYG4iHoCAr8ObpxDXL_7551HUUO7JEpT3JuchiU2HlcQDrhwcduOJ4bF-1d1D4ZO8lWOmw/file# [following]\n",
            "--2022-03-12 13:16:40--  https://uc67352ba0fb49de598d86077b6c.dl.dropboxusercontent.com/cd/0/inline/BhXO1eiiIJpaZcHwAKnJtDR_3hcJHj653piyLB3Qy6U5yM5TR4wUpxV0UxxBKy8H2cRDmgc22E727ZKEK4fPmsd3U7k5dCshLUkNqNxojYG4iHoCAr8ObpxDXL_7551HUUO7JEpT3JuchiU2HlcQDrhwcduOJ4bF-1d1D4ZO8lWOmw/file\n",
            "Resolving uc67352ba0fb49de598d86077b6c.dl.dropboxusercontent.com (uc67352ba0fb49de598d86077b6c.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc67352ba0fb49de598d86077b6c.dl.dropboxusercontent.com (uc67352ba0fb49de598d86077b6c.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BhUMNFgMRW1R8blrMHunALj91XDwFOcQSjr2yiaeUO97fe2P-kJdNMcETaGuuHFOkFswuXQ1bGpivOJ0YPCa2zAcy74-NQsPJgpKW5j9J_eaDlTT2_R-rFZFna69v0QFA1sfjSMvUei7b7B9btMJG2crxtrB75OlVolPdm7jl_Jr_jzh0FIds-vnyi7hfOOE_6CEOqLKB0Rz6TDphu_yHiCDfpHW20bRZ8zQMvl-iNt_-X4UQ4J6u5WUZyHI38V-m4H6cOAVHkx9riTGDF-fewzE3Bna2oTxd1LEIqbnCqNWxQbzgw8elbH8zI54J9xtbRs0dVQEwL2bencOIQU__YbvbTomtozPjhg9MeAWdliFBx_7spOlzyMRp7g2VmeCdEzm4wShAic2K41PcyOZYx3Iutny9gk6yZ1upjtMx_urQQ/file [following]\n",
            "--2022-03-12 13:16:40--  https://uc67352ba0fb49de598d86077b6c.dl.dropboxusercontent.com/cd/0/inline2/BhUMNFgMRW1R8blrMHunALj91XDwFOcQSjr2yiaeUO97fe2P-kJdNMcETaGuuHFOkFswuXQ1bGpivOJ0YPCa2zAcy74-NQsPJgpKW5j9J_eaDlTT2_R-rFZFna69v0QFA1sfjSMvUei7b7B9btMJG2crxtrB75OlVolPdm7jl_Jr_jzh0FIds-vnyi7hfOOE_6CEOqLKB0Rz6TDphu_yHiCDfpHW20bRZ8zQMvl-iNt_-X4UQ4J6u5WUZyHI38V-m4H6cOAVHkx9riTGDF-fewzE3Bna2oTxd1LEIqbnCqNWxQbzgw8elbH8zI54J9xtbRs0dVQEwL2bencOIQU__YbvbTomtozPjhg9MeAWdliFBx_7spOlzyMRp7g2VmeCdEzm4wShAic2K41PcyOZYx3Iutny9gk6yZ1upjtMx_urQQ/file\n",
            "Reusing existing connection to uc67352ba0fb49de598d86077b6c.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6640657 (6.3M) [application/zip]\n",
            "Saving to: ‘NYT11.zip’\n",
            "\n",
            "NYT11.zip           100%[===================>]   6.33M  31.5MB/s    in 0.2s    \n",
            "\n",
            "2022-03-12 13:16:41 (31.5 MB/s) - ‘NYT11.zip’ saved [6640657/6640657]\n",
            "\n",
            "replace dev_triples.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace test_triples.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace train_triples.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace rel2id.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 数据"
      ],
      "metadata": {
        "id": "5I5gKAEMBdCC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qunlH1kqcZs7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "09415d26246e436f8b1a5db09477fe8b",
            "a30bea0eb51a49aab83fba7cf62611af",
            "e064e27eeb674f20816a52739150ce3d",
            "6a675ac2d08f4724a9a78c8a9d9ae8bf",
            "119edc1656ef4d2597ec96b4630ca7c6",
            "4c77e2e5a3bf4f50a80b04bcef3f5505",
            "e45a0b21d4104f348ea3b0a08e7288ef",
            "fd737be345fd47e6bd2522f5c68cb704",
            "d2b8a2044e4a46479ad82c3180b3ec15",
            "cb82c6df27054a7d917d14205addb6cd",
            "b614ac2c203a42f1a595108f36b5d4e4",
            "fa8a3f0d6e01477ba77a1a234588370c",
            "dbdaa1a798e9428b899f2e50d298d0aa",
            "36f2a713972a48b5a6f85f62bcb50ffe",
            "2cb84df0e6b943e5aae371884179406a",
            "eb95445520c747169a25781674a0b512",
            "776e34674d8143638a7b6209933d6a17",
            "c7c373ae955f431c8d0f6840348a31c4",
            "b1dea582dcaa43579871f914637bb743",
            "1584bf3417e343e9abdde2334b07bccb",
            "025ff577ef554b549de7429d96b2159a",
            "590c89d14af24a659c7ec989551933ad",
            "acced878c7394853a4fbaf356f869854",
            "f8fce8b73cdb43658b45e6df6d2afe07",
            "c7a4ec0c9ddc420897b5192be20802ed",
            "8090e57f7fb144839d05432b72460c21",
            "0ddae0562a53494295a708cb45dad70a",
            "f690c4f1618b492db34414625189fb65",
            "a0dd260faf864d8ab9b20aa17c2f04b1",
            "a9fa3f7abba048869463c5cbf9137afa",
            "2bcf37ece81e4771941724390c6427a1",
            "a9695a18c64d4d7d8b249d697693c505",
            "9a93bc8471f346a8b01fa4ec6a619e97"
          ]
        },
        "outputId": "eab6ae7e-8b62-4fbd-9413-322d5770987c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_data len: 62335\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09415d26246e436f8b1a5db09477fe8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa8a3f0d6e01477ba77a1a234588370c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acced878c7394853a4fbaf356f869854"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_data len: 313\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    test_data, id2rel, rel2id, num_rels=load_data('./train_triples.json',rel_dict_path='./rel2id.json')\n",
        "    tokenizer=BertTokenizer.from_pretrained(model_name)\n",
        "    maxlen=100\n",
        "    val_data,dev_id2rel, dev_rel2id, dev_num_rels=load_data('./dev_triples.json',rel_dict_path='./rel2id.json')\n",
        "    tokens_batch, segments_batch, sub_heads_batch, sub_tails_batch, sub_head_batch, sub_tail_batch, obj_heads_batch, obj_tails_batch=data_generator(test_data, tokenizer, rel2id, num_rels, maxlen)\\\n",
        "        .generator()\n",
        "    dev_tokens,dev_segs,dev_heads,dev_tails,dev_head,dev_tail,dev_oheads,dev_otails=data_generator(val_data, tokenizer, rel2id, num_rels, maxlen).generator()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_tails_batch[:5]"
      ],
      "metadata": {
        "id": "4r4Vvsw4BftP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f832e6-cafb-4766-a1cf-b150387da90b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0.]),\n",
              " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0.]),\n",
              " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0.]),\n",
              " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0.])]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens_batch)"
      ],
      "metadata": {
        "id": "FgNtE9DY65lv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5310ec5b-17a2-4f47-b293-a6fdae39b1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62335"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_batch=torch.cat([l for l in tokens_batch]).int()\n",
        "print(tokens_batch.shape)\n",
        "segments_batch=torch.cat([l for l in segments_batch]).int()#sents_length*128\n",
        "sub_heads_batch=torch.cat([l for l in sub_heads_batch]).reshape(tokens_batch.shape[0],-1).float()#128\n",
        "sub_tails_batch=torch.cat([l for l in sub_tails_batch]).reshape(tokens_batch.shape[0],-1).float()\n",
        "obj_heads_batch=torch.cat([l for l in obj_heads_batch]).reshape(tokens_batch.shape[0],128,-1).float()\n",
        "obj_tails_batch=torch.cat([l for l in obj_tails_batch]).reshape(tokens_batch.shape[0],128,-1).float()\n",
        "print(type(segments_batch))\n",
        "print(segments_batch.shape)\n",
        "print(obj_heads_batch.shape)\n",
        "print(type(obj_heads_batch))\n",
        "dev_tokens=torch.cat([l for l in dev_tokens]).int()\n",
        "dev_segs=torch.cat([l for l in dev_segs]).int()\n",
        "dev_heads=torch.cat([l for l in dev_heads]).reshape(dev_tokens.shape[0],-1).float()\n",
        "dev_tails=torch.cat([l for l in dev_tails]).reshape(dev_tokens.shape[0],-1).float()\n",
        "dev_oheads=torch.cat([l for l in dev_oheads]).reshape(dev_tokens.shape[0],128,-1).float()\n",
        "dev_otails=torch.cat([l for l in dev_otails]).reshape(dev_tokens.shape[0],128,-1).float()"
      ],
      "metadata": {
        "id": "1DAkVUup6xrW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09260714-716e-433c-c438-0e4d4b15cce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([62335, 128])\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([62335, 128])\n",
            "torch.Size([62335, 128, 12])\n",
            "<class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_batch.shape[0]"
      ],
      "metadata": {
        "id": "2SIMSL8B6BmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad5aba7-43ba-4b2e-df9b-0d5207ea36e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62335"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_data)"
      ],
      "metadata": {
        "id": "eQxieP-Z6KAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc1d3ee9-c260-453e-95f5-a287cbb79d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62335"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_data)"
      ],
      "metadata": {
        "id": "9SM5nrdB56OY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "340d0951-e1df-41b3-8a91-a87e720d02ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "313"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UwnFteQk4W2j"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# tokens = tokenizer.tokenize('Jackie')\n",
        "# tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qThX2MDs684L"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# line=test_data[0]\n",
        "# line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MnwXTHnQ7Mg8"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# text = ' '.join(line['text'].split()[:100])\n",
        "# tokens =tokenizer.tokenize(text)\n",
        "# print(text,'\\n',tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n--BAwE7_0ZT"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# inputs=tokenizer(text,return_tensors='pt',add_special_tokens=False,truncation=True,padding=True,max_length=BERT_MAX_LEN)\n",
        "# token_ids, segment_ids=inputs['input_ids'],inputs['attention_mask']\n",
        "# print(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JtPmgAiT33R5"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# tokens_batch[0]#10*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z3XV1uia36hg"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# segments_batch[0]#46\n",
        "# sub_heads_batch[0]#18*\n",
        "# sub_tails_batch[0]\n",
        "# obj_heads_batch[0]\n",
        "# obj_tails_batch[[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meua4E7CfGVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9684e623-0e53-478e-b250-5881f188e031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([62335, 128])\n",
            "torch.Size([62335, 128, 12])\n",
            "<class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(type(segments_batch))\n",
        "print(segments_batch.shape)\n",
        "print(obj_heads_batch.shape)\n",
        "print(type(obj_heads_batch))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_heads_batch[:3]"
      ],
      "metadata": {
        "id": "U4rvqm4QHw7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a945cde6-39f9-4c60-d658-dfe812b251a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "         0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_tails_batch.shape"
      ],
      "metadata": {
        "id": "PGIwd2EhH0ny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ffe56b-ea0b-4806-ee25-bfa2e54c4e41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([62335, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4ukp7gPfQ7U"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,Dataset,TensorDataset\n",
        "train_set=TensorDataset(tokens_batch, segments_batch, sub_heads_batch, sub_tails_batch,obj_heads_batch,obj_tails_batch)\n",
        "from torch.nn import functional as F\n",
        "dev_set=TensorDataset(dev_tokens,dev_segs,dev_heads,dev_tails,dev_oheads,dev_otails)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yjCK6NNVWMFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJXxOdkgfXKD"
      },
      "source": [
        "### seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUhn6YP6fU71"
      },
      "outputs": [],
      "source": [
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    # np.random.seed(seed)\n",
        "    # random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "same_seeds(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBhVxOhofb8F"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g7EfbyDfa7j"
      },
      "outputs": [],
      "source": [
        "class E2EModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(E2EModel, self).__init__()\n",
        "        self.label_num=len(rel2id)\n",
        "        self.encode=BertModel.from_pretrained(model_name)\n",
        "        self.subject_h_model=nn.Sequential(\n",
        "          nn.Linear(768, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.subject_t_model=nn.Sequential(\n",
        "            nn.Linear(768, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.object_h_model=nn.Sequential(\n",
        "            nn.Linear(768, self.label_num),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.object_t_model = nn.Sequential(\n",
        "            nn.Linear(768, self.label_num),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,inputs_id,att_mask,sub_tar,tail_tar,obj_htar,obj_ttar):\n",
        "        x=self.encode(inputs_id,att_mask)[0]#B*L*768\n",
        "        sub_tar=sub_tar.unsqueeze(-1)\n",
        "        tail_tar=tail_tar.unsqueeze(-1)\n",
        "        sub=self.subject_h_model(x)#B*L*1\n",
        "        tail=self.subject_t_model(x)\n",
        "        sub_loss=F.binary_cross_entropy(sub,sub_tar)+F.binary_cross_entropy(tail,tail_tar)\n",
        "        subfea=sub+tail#这里与原文有些不符\n",
        "        objinput=subfea*x+x\n",
        "        obj_h=self.object_h_model(objinput)\n",
        "        obj_t=self.object_t_model(objinput)\n",
        "        obj_loss=F.binary_cross_entropy(obj_h,obj_htar)+F.binary_cross_entropy(obj_t,obj_ttar)\n",
        "        total_loss=obj_loss+sub_loss\n",
        "        return sub,tail,obj_h,obj_t, sub_loss,obj_loss,total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 修改"
      ],
      "metadata": {
        "id": "HKo4eiiuXOs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(pred,gold,mask): \n",
        "  loss = F.binary_cross_entropy(pred, gold, reduction='none')\n",
        "  if loss.shape != mask.shape:\n",
        "    mask = mask.unsqueeze(-1)\n",
        "    loss = torch.sum(loss * mask) / torch.sum(mask)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "G03FR5aFXZHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xlVYP9eXMtZ"
      },
      "outputs": [],
      "source": [
        "class E2EModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(E2EModel, self).__init__()\n",
        "        self.label_num=len(rel2id)\n",
        "        self.encode=BertModel.from_pretrained(model_name)\n",
        "        self.subject_h_model=nn.Sequential(\n",
        "          nn.Linear(768, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.subject_t_model=nn.Sequential(\n",
        "            nn.Linear(768, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.object_h_model=nn.Sequential(\n",
        "            nn.Linear(768, self.label_num),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.object_t_model = nn.Sequential(\n",
        "            nn.Linear(768, self.label_num),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,inputs_id,att_mask,sub_tar,tail_tar,obj_htar,obj_ttar):\n",
        "        x=self.encode(inputs_id,att_mask)[0]#B*L*768\n",
        "        sub_tar=sub_tar.unsqueeze(-1)\n",
        "        tail_tar=tail_tar.unsqueeze(-1)\n",
        "        sub=self.subject_h_model(x)#B*L*1\n",
        "        tail=self.subject_t_model(x)\n",
        "        sub_loss=loss_fn(sub,sub_tar,att_mask)+loss_fn(tail,tail_tar,att_mask)\n",
        "        subfea=sub+tail#这里与原文有些不符\n",
        "        objinput=subfea*x+x\n",
        "        obj_h=self.object_h_model(objinput)\n",
        "        obj_t=self.object_t_model(objinput)\n",
        "        obj_loss=loss_fn(obj_h,obj_htar,att_mask)+loss_fn(obj_t,obj_ttar,att_mask)\n",
        "        total_loss=obj_loss+sub_loss\n",
        "        return sub,tail,obj_h,obj_t, sub_loss,obj_loss,total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 修改——苏剑林（model_cnn）"
      ],
      "metadata": {
        "id": "sMNq94eVAhZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class subModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(subModel, self).__init__()\n",
        "#         self.label_num=len(rel2id)\n",
        "#         self.encode=BertModel.from_pretrained(model_name)\n",
        "#         self.sub_conv_1=nn.Conv1d(768,256,3,1,padding=1)\n",
        "#         self.sub_conv_2=nn.Conv1d(768,256,3,1,padding=1)\n",
        "#         self.obj_conv_1=nn.Conv1d(768,256,3,1,padding=1)\n",
        "#         self.obj_conv_2=nn.Conv1d(768,256,3,1,padding=1)\n",
        "#         self.subject_h_model=nn.Sequential(\n",
        "#             nn.Linear(256, 1),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "#         self.subject_t_model=nn.Sequential(\n",
        "#             nn.Linear(256, 1),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "#         self.object_h_model = nn.Sequential(\n",
        "#             nn.Linear(256, self.label_num),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "#         self.object_t_model = nn.Sequential(\n",
        "#             nn.Linear(256, self.label_num),\n",
        "#             nn.Sigmoid()\n",
        "#         )\n",
        "\n",
        "\n",
        "\n",
        "#     def forward(self, inputs_id, att_mask, sub_tar, tail_tar, obj_htar, obj_ttar):\n",
        "#             x = self.encode(inputs_id, att_mask)[0]  # B*L*768\n",
        "#             sub_tar = sub_tar.unsqueeze(-1)\n",
        "#             tail_tar = tail_tar.unsqueeze(-1)\n",
        "#             x=x.transpose(2,1)#B*768*L\n",
        "#             sub_x_1=self.sub_conv_1(x)\n",
        "#             sub_x_2=self.sub_conv_2(x)\n",
        "#             sub = self.subject_h_model(sub_x_1.transpose(2,1))  # B*L*1\n",
        "#             tail = self.subject_t_model(sub_x_2.transpose(2,1))\n",
        "#             sub_loss = F.binary_cross_entropy(sub, sub_tar) + F.binary_cross_entropy(tail, tail_tar)\n",
        "#             subfea = sub + tail  # 这里与原文有些不符\n",
        "#             objinput = subfea * x.transpose(2,1) + x.transpose(2,1)\n",
        "#             objinput=objinput.transpose(2,1)\n",
        "#             obj_x_1=self.obj_conv_1(objinput)\n",
        "#             obj_x_2=self.obj_conv_2(objinput)\n",
        "#             obj_h = self.object_h_model(obj_x_1.transpose(2,1))\n",
        "#             obj_t = self.object_t_model(obj_x_2.transpose(2,1))\n",
        "#             obj_loss = F.binary_cross_entropy(obj_h, obj_htar) + F.binary_cross_entropy(obj_t, obj_ttar)\n",
        "#             total_loss = obj_loss + sub_loss\n",
        "#             return sub, tail, obj_h, obj_t, sub_loss, obj_loss, total_loss\n",
        "\n",
        "# model=subModel()\n",
        "# model_path='./submodel.ckpt'"
      ],
      "metadata": {
        "id": "pflCpJdFMCth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # model.bert_model(torch.from_numpy(np.array(train_id[:4])),torch.from_numpy(np.array(train_mask[:4])))\n",
        "\n",
        "# optim=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "# #\n",
        "# import transformers\n",
        "\n",
        "# # for epoch in range(EPOCHS):\n",
        "# #     step=1\n",
        "# #     model.train()\n",
        "# #     for i,data in tqdm(enumerate(train_loader)):\n",
        "# #         input_id,input_mask,label=data\n",
        "# #         pre=model.forward2(input_id,input_mask)\n",
        "# #         loss=criterier(pre,label)\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "Q05nFpdvAx4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 文中的模型"
      ],
      "metadata": {
        "id": "5DutQU6aU-_q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDYOGhSrfmrD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149,
          "referenced_widgets": [
            "72f577a67792411ab0b0b1339a61692a",
            "b0fb66c5b719424b9f7a5839542a88df",
            "17a37f9c8f5746429b8d25d7b039045d",
            "710403b1e54248079911cf2a7e6e3647",
            "c0339597596c4d42bc39dd15607b1878",
            "f3835ac47fbb4fe6b03dcf8c4d2dce75",
            "992624383aef4454ab84938367a7c8d6",
            "6392d2cae7304c7f922630a619cc397a",
            "2caab8d9c236430391dbff71b4af45fa",
            "427f56242a4043599122f7552298c142",
            "873821f35d464ba89e2f5ee8fabc862c"
          ]
        },
        "outputId": "658442d6-edf5-451f-c42e-46aa07a3b7c1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72f577a67792411ab0b0b1339a61692a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model=E2EModel()\n",
        "model_path='./model.ckpt'\n",
        "# model.bert_model(torch.from_numpy(np.array(train_id[:4])),torch.from_numpy(np.array(train_mask[:4])))\n",
        "\n",
        "optim=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "\n",
        "import transformers\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#     step=1\n",
        "#     model.train()\n",
        "#     for i,data in tqdm(enumerate(train_loader)):\n",
        "#         input_id,input_mask,label=data\n",
        "#         pre=model.forward2(input_id,input_mask)\n",
        "#         loss=criterier(pre,label)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSfWxrYWfsdZ"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def partial_match(pred_set, gold_set):\n",
        "    pred = {(i[0].split(' ')[0] if len(i[0].split(' ')) > 0 else i[0], i[1],\n",
        "                 i[2].split(' ')[0] if len(i[2].split(' ')) > 0 else i[2]) for i in pred_set}\n",
        "    gold = {(i[0].split(' ')[0] if len(i[0].split(' ')) > 0 else i[0], i[1],\n",
        "                 i[2].split(' ')[0] if len(i[2].split(' ')) > 0 else i[2]) for i in gold_set}\n",
        "    return pred, gold"
      ],
      "metadata": {
        "id": "EmMwpg2CxsK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "v8jZ-XWat4jd"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def metric(eval_data,exact_match=False, output_path=None):\n",
        "    if output_path:\n",
        "        F = open(output_path, 'w+')\n",
        "    orders = ['subject', 'relation', 'object']\n",
        "    correct_num, predict_num, gold_num = 1e-10, 1e-10, 1e-10\n",
        "    # for line in tqdm(iter(eval_data)):\n",
        "    with open('triple_list.json') as f:\n",
        "        f=f.readline()\n",
        "        dict_=json.loads(f)\n",
        "    for i,line in enumerate(eval_data):\n",
        "        if i<dev_tokens.shape[0]:\n",
        "            pre=dict_[str(i)]\n",
        "            Pred_triples=set([tuple(l) for l in pre])\n",
        "            Gold_triples = set(tuple(l) for l in line['triple_list'])\n",
        "            Pred_triples_eval, Gold_triples_eval = partial_match(Pred_triples, Gold_triples) if not exact_match else (Pred_triples, Gold_triples)\n",
        "\n",
        "            correct_num += len(Pred_triples_eval & Gold_triples_eval)\n",
        "            predict_num += len(Pred_triples_eval)\n",
        "            gold_num += len(Gold_triples_eval)\n",
        "\n",
        "            if output_path:\n",
        "                result = json.dumps({\n",
        "                    'text': line['text'],\n",
        "                    'triple_list_gold': [\n",
        "                        dict(zip(orders, triple)) for triple in Gold_triples\n",
        "                    ],\n",
        "                    'triple_list_pred': [\n",
        "                        dict(zip(orders, triple)) for triple in Pred_triples\n",
        "                    ],\n",
        "                    'new': [\n",
        "                        dict(zip(orders, triple)) for triple in Pred_triples - Gold_triples\n",
        "                    ],\n",
        "                    'lack': [\n",
        "                        dict(zip(orders, triple)) for triple in Gold_triples - Pred_triples\n",
        "                    ]\n",
        "                }, ensure_ascii=False, indent=4)\n",
        "                F.write(result + '\\n')\n",
        "\n",
        "\n",
        "    precision = correct_num / predict_num\n",
        "    recall = correct_num / gold_num\n",
        "    f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    print(f'correct_num:{correct_num}\\npredict_num:{predict_num}\\ngold_num:{gold_num}')\n",
        "    return precision, recall, f1_score\n",
        "def partial_match(pred_set, gold_set):\n",
        "    pred = {(i[0].split(' ')[0] if len(i[0].split(' ')) > 0 else i[0], i[1],\n",
        "                 i[2].split(' ')[0] if len(i[2].split(' ')) > 0 else i[2]) for i in pred_set}\n",
        "    gold = {(i[0].split(' ')[0] if len(i[0].split(' ')) > 0 else i[0], i[1],\n",
        "                 i[2].split(' ')[0] if len(i[2].split(' ')) > 0 else i[2]) for i in gold_set}\n",
        "    return pred, gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEvBGePWY5Dz"
      },
      "outputs": [],
      "source": [
        "def metric(eval_data,exact_match=False, output_path=None):\n",
        "    if output_path:\n",
        "        F = open(output_path, 'w+')\n",
        "    orders = ['subject', 'relation', 'object']\n",
        "    correct_num, predict_num, gold_num = 1e-10, 1e-10, 1e-10\n",
        "    # for line in tqdm(iter(eval_data)):\n",
        "    with open('val_triples.json') as f:\n",
        "        f=f.readline()\n",
        "        dict_=json.loads(f)\n",
        "    for i,line in enumerate(eval_data):\n",
        "        if i<tokens_batch.shape[0]:\n",
        "            pre=dict_[str(i)]\n",
        "            Pred_triples=set([tuple(l) for l in pre])\n",
        "            Gold_triples = set(tuple(l) for l in line['triple_list'])\n",
        "            Pred_triples_eval, Gold_triples_eval = partial_match(Pred_triples, Gold_triples) if not exact_match else (Pred_triples, Gold_triples)\n",
        "\n",
        "            correct_num += len(Pred_triples_eval & Gold_triples_eval)\n",
        "            predict_num += len(Pred_triples_eval)\n",
        "            gold_num += len(Gold_triples_eval)\n",
        "\n",
        "            # if output_path:\n",
        "            #     result = json.dumps({\n",
        "            #         'text': line['text'],\n",
        "            #         'triple_list_gold': [\n",
        "            #             dict(zip(orders, triple)) for triple in Gold_triples\n",
        "            #         ],\n",
        "            #         'triple_list_pred': [\n",
        "            #             dict(zip(orders, triple)) for triple in Pred_triples\n",
        "            #         ],\n",
        "            #         'new': [\n",
        "            #             dict(zip(orders, triple)) for triple in Pred_triples - Gold_triples\n",
        "            #         ],\n",
        "            #         'lack': [\n",
        "            #             dict(zip(orders, triple)) for triple in Gold_triples - Pred_triples\n",
        "            #         ]\n",
        "            #     }, ensure_ascii=False, indent=4)\n",
        "            #     F.write(result + '\\n')\n",
        "            if output_path:\n",
        "                result = json.dumps({'text': line['text'],'triple_list_gold': [dict(zip(orders, triple)) for triple in Gold_triples],'triple_list_pred': [dict(zip(orders, triple)) for triple in Pred_triples],'new': [dict(zip(orders, triple)) for triple in Pred_triples - Gold_triples],'lack': [dict(zip(orders, triple)) for triple in Gold_triples - Pred_triples]}, ensure_ascii=False)\n",
        "\n",
        "                F.write(result + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "    precision = correct_num / predict_num\n",
        "    recall = correct_num / gold_num\n",
        "    f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    print(f'correct_num:{correct_num}\\npredict_num:{predict_num}\\ngold_num:{gold_num}')\n",
        "    return precision, recall, f1_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extrac_triple(sub_heads_logits, sub_tails_logits, obj_heads_logits, obj_tails_logits, input_ids):  # 抽取一个triple\n",
        "    h_bar = 0.5\n",
        "    t_bar = 0.5\n",
        "    sub_heads_logits = np.array(sub_heads_logits)\n",
        "    sub_tails_logits = np.array(sub_tails_logits)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    # print(tokens)\n",
        "    sub_heads, sub_tails = np.where(sub_heads_logits > h_bar)[0], np.where(sub_tails_logits > t_bar)[0]\n",
        "    subjects = []\n",
        "    for sub_head in sub_heads:\n",
        "        sub_tail = sub_tails[sub_tails >= sub_head]\n",
        "        if len(sub_tail) > 0:\n",
        "            sub_tail = sub_tail[0]\n",
        "            if sub_tail == sub_head:\n",
        "                subject = tokens[sub_head]\n",
        "                subjects.append((subject, sub_head, sub_tail))\n",
        "            else:\n",
        "                subject = tokens[sub_head: sub_tail]\n",
        "                subjects.append((subject, sub_head, sub_tail))\n",
        "    if subjects:\n",
        "        triple_list = []\n",
        "        sub_heads, sub_tails = np.array([sub[1:] for sub in subjects]).T.reshape((2, -1, 1))\n",
        "        for i, subject in enumerate(subjects):\n",
        "            sub = subject[0]\n",
        "            sub = ''.join([i.lstrip(\"##\") for i in sub])\n",
        "            sub = ' '.join(sub.split('[unused1]'))\n",
        "            obj_heads, obj_tails = np.where(obj_heads_logits > h_bar), np.where(obj_tails_logits > t_bar)\n",
        "            for obj_head, rel_head in zip(*obj_heads):\n",
        "                for obj_tail, rel_tail in zip(*obj_tails):\n",
        "                    if obj_head <= obj_tail and rel_head == rel_tail:\n",
        "                        rel = id2rel[rel_head]\n",
        "                        if obj_head == obj_tail:\n",
        "                            obj = tokens[obj_head]\n",
        "                        else:\n",
        "                            obj = tokens[obj_head: obj_tail]\n",
        "                        obj = ''.join([i.lstrip(\"##\") for i in obj])\n",
        "                        obj = ' '.join(obj.split('[unused1]'))\n",
        "                        triple_list.append((sub, rel, obj))\n",
        "                        break\n",
        "        triple_set = set()\n",
        "        for s, r, o in triple_list:\n",
        "            triple_set.add((s, r, o))\n",
        "        return list(triple_set)\n",
        "    else:\n",
        "        return []"
      ],
      "metadata": {
        "id": "D1dZEpV_w3yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "warmup_steps = 2500\n",
        "init_lr = 0.1  \n",
        "# 模拟训练15000步\n",
        "max_steps = 15000\n",
        "for train_steps in range(max_steps):\n",
        "    if warmup_steps and train_steps < warmup_steps:\n",
        "        warmup_percent_done = train_steps / warmup_steps\n",
        "        warmup_learning_rate = init_lr * warmup_percent_done  #gradual warmup_lr\n",
        "        learning_rate = warmup_learning_rate\n",
        "    else:\n",
        "        #learning_rate = np.sin(learning_rate)  #预热学习率结束后,学习率呈sin衰减\n",
        "        learning_rate = learning_rate**1.0001 #预热学习率结束后,学习率呈指数衰减(近似模拟指数衰减)\n"
      ],
      "metadata": {
        "id": "NE8bmiyihbGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmZBC7aPtx2p"
      },
      "outputs": [],
      "source": [
        "# from sys import last_traceback\n",
        "\n",
        "def train(net, train_set,val_set, num_epochs, learning_rate, batch_size):\n",
        "    i = 1\n",
        "    print('执行次数为：{}'.format(i))\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "    t_total = len(train_loader) // num_epochs\n",
        "    \n",
        "    \n",
        "    # scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=t_total)\n",
        "\n",
        "    net = net.to(device)\n",
        "    net.train()\n",
        "    # optimizer = AdamW(net.parameters(), lr=learning_rate)\n",
        "    # criterier = nn.CrossEntropyLoss()\n",
        "    # criterier=nn.MSELoss()\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss, sub_hacc,sub_tacc,obj_hacc,obj_tacc = 0, 0,0,0,0\n",
        "        accum_iter = 4\n",
        "        step = 0\n",
        "        for batch_idx, data in enumerate(tqdm(train_loader)):\n",
        "            lr=learning_rate\n",
        "            optimizer = torch.optim.Adam(net.parameters(), lr=lr,betas=[0.9,0.9])\n",
        "            inputs_id,att_mask,sub_htar,sub_ttar,obj_htar,obj_ttar = data\n",
        "            text, mask, sub_htar,sub_ttar,obj_htar,obj_ttar =inputs_id.to(device),att_mask.to(device),sub_htar.to(device),sub_ttar.to(device),obj_htar.to(device),obj_ttar.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            # with torch.set_grad_enabled(True):\n",
        "            sub,tail,obj_h,obj_t, sub_loss,obj_loss,total_loss= net(text, mask, sub_htar,sub_ttar,obj_htar,obj_ttar)\n",
        "            sub_htar,sub_ttar=sub_htar.unsqueeze(-1),sub_ttar.unsqueeze(-1)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "            sub_hacc+=(sub.ge(0.5).int()==sub_htar).sum().item()\n",
        "            sub_tacc += (tail.ge(0.5).int() == sub_ttar).sum().item()\n",
        "            obj_hacc+=(obj_h.ge(0.5).int()==obj_htar).sum().item()\n",
        "            obj_tacc+=(obj_t.ge(0.5).int()==obj_ttar).sum().item()\n",
        "            train_loss += total_loss.item()\n",
        "\n",
        "            step += 1\n",
        "            if step % 800 == 0:\n",
        "                print('train_epoch|{},sub_loss={},sub_h_acc={}，sub_t_acc={}'.format(epoch+1, sub_loss,sub_hacc/(step*batch_size*128),\n",
        "                                                                                sub_tacc/(step*batch_size*128)))\n",
        "                print('obj_loss={},obj_h_acc={}，obj_t_acc={}'.format(obj_loss,obj_hacc/(step*batch_size*128*len(rel2id)),\n",
        "                                                                                obj_tacc/(step*batch_size*128*len(rel2id))))\n",
        "                print('epoch{}|train_loss={},total_loss={}'.format(epoch+1,train_loss,train_loss/step))\n",
        "        torch.save(net.state_dict(), model_path)\n",
        "\n",
        "        net.eval()\n",
        "        val_loader = DataLoader(val_set,batch_size=1,shuffle=False)\n",
        "        val_batch_size=1\n",
        "        with torch.no_grad():\n",
        "            dict_t = {}\n",
        "            vstep = 0\n",
        "            train_loss, sub_hacc, sub_tacc, obj_hacc, obj_tacc = 0, 0, 0, 0, 0\n",
        "            f = open('val_triples.json', mode='w+', encoding='utf_8')\n",
        "          \n",
        "            for j, data in enumerate(val_loader):\n",
        "                inputs_id, att_mask, sub_htar, sub_ttar, obj_htar, obj_ttar = data\n",
        "                text, mask, sub_htar, sub_ttar, obj_htar, obj_ttar = inputs_id.to(device), att_mask.to(device), sub_htar.to(\n",
        "                    device), sub_ttar.to(device), obj_htar.to(device), obj_ttar.to(device)\n",
        "                sub, tail, obj_h, obj_t, sub_loss, obj_loss, total_loss = model(text, mask, sub_htar, sub_ttar, obj_htar,\n",
        "                                                                                obj_ttar)\n",
        "                sub_h = sub.ge(0.5).int()\n",
        "                sub_t = tail.ge(0.5).int()\n",
        "                obj_h = obj_h.ge(0.5).int()\n",
        "                obj_t = obj_t.ge(0.5).int()\n",
        "                sub_hacc += (sub.ge(0.5).int() == sub_htar).sum().item()\n",
        "                sub_tacc += (tail.ge(0.5).int() == sub_ttar).sum().item()\n",
        "                obj_hacc += (obj_h.ge(0.5).int() == obj_htar).sum().item()\n",
        "                obj_tacc += (obj_t.ge(0.5).int() == obj_ttar).sum().item()\n",
        "                train_loss += total_loss.item()\n",
        "                vstep += 1\n",
        "                sub, tail, obj_h, obj_t = sub.cpu(), tail.cpu(), obj_h.cpu(), obj_t.cpu()\n",
        "                sub_heads_logits, sub_tails_logits, obj_heads_logits, obj_tails_logits = np.array(sub), np.array(\n",
        "                    tail), np.array(obj_h), np.array(obj_t)\n",
        "                inputs_id = inputs_id.cpu()\n",
        "                # for i in range(val_batch_size):\n",
        "                #     sub_heads_logits, sub_tails_logits, obj_heads_logits, obj_tails_logits=sub_heads_logits[i],sub_tails_logits[i],obj_heads_logits[i],obj_tails_logits[i]\n",
        "                #     input_ids=inputs_id[i]\n",
        "                #     triple_list=extrac_triple(sub_heads_logits,sub_tails_logits,obj_heads_logits,obj_tails_logits,input_ids)\n",
        "                #     dict_t[j]=triple_list\n",
        "                triple_list = extrac_triple(sub_heads_logits[0], sub_tails_logits[0], obj_heads_logits[0], obj_tails_logits[0], inputs_id[0])\n",
        "                dict_t[j] = triple_list\n",
        "                # if vstep<20:\n",
        "                #   tokens = tokenizer.convert_ids_to_tokens(inputs_id[0])\n",
        "                #   print(tokens)\n",
        "                #   print('\\n')\n",
        "                #   print(triple_list)\n",
        "            # print(dict_t)\n",
        "            json.dump(dict_t, f)\n",
        "            f.close()\n",
        "            print('-' * 50)\n",
        "            print('val_train_epoch|{},val_sub_loss={},sub_h_acc={}，sub_t_acc={}'.format( 1, sub_loss, sub_hacc / (\n",
        "                        len(dev_set) * val_batch_size * 128),\n",
        "                                                                                        sub_tacc / (\n",
        "                                                                                                    len(dev_set) * val_batch_size * 128)))\n",
        "            print('obj_loss={},obj_h_acc={}，obj_t_acc={}'.format(obj_loss, obj_hacc / (\n",
        "                        len(dev_set) * val_batch_size * 128 * len(rel2id)),\n",
        "                                                                obj_tacc / (len(dev_set) * val_batch_size * 128 * len(\n",
        "                                                                    rel2id))))\n",
        "            print('val_epoch{}|total_loss={}'.format( 1, train_loss / vstep))\n",
        "        precision, recall, f1_score = metric(val_data, exact_match=False, output_path='valoutputs.json')\n",
        "        print(f'precision={precision},recall={recall},f1_score={f1_score}')        \n",
        "        # with torch.no_grad():\n",
        "        #     dict_t = {}\n",
        "        #     vstep=0\n",
        "        #     train_loss, sub_hacc, sub_tacc, obj_hacc, obj_tacc = 0, 0, 0, 0, 0\n",
        "        #     f=open('val_traintriple_list.json',mode='w+',encoding='utf_8')\n",
        "        #     for j, data in enumerate(val_loader):\n",
        "        #         inputs_id, att_mask, sub_htar, sub_ttar, obj_htar, obj_ttar = data\n",
        "        #         text, mask, sub_htar, sub_ttar, obj_htar, obj_ttar = inputs_id.to(device), att_mask.to(device), sub_htar.to(\n",
        "        #             device), sub_ttar.to(device), obj_htar.to(device), obj_ttar.to(device)\n",
        "        #         sub, tail, obj_h, obj_t, sub_loss, obj_loss, total_loss = model(text, mask, sub_htar, sub_ttar, obj_htar,obj_ttar)\n",
        "        #         sub_h=sub.ge(0.5).int()\n",
        "        #         sub_t=tail.ge(0.5).int()\n",
        "        #         obj_h=obj_h.ge(0.5).int()\n",
        "        #         obj_t=obj_t.ge(0.5).int()\n",
        "        #         sub_hacc += (sub.ge(0.5).int() == sub_htar).sum().item()\n",
        "        #         sub_tacc += (tail.ge(0.5).int() == sub_ttar).sum().item()\n",
        "        #         obj_hacc += (obj_h.ge(0.5).int() == obj_htar).sum().item()\n",
        "        #         obj_tacc += (obj_t.ge(0.5).int() == obj_ttar).sum().item()\n",
        "        #         train_loss += total_loss.item()\n",
        "        #         vstep+=1\n",
        "        #         sub, tail, obj_h, obj_t=sub.cpu(),tail.cpu(),obj_h.cpu(),obj_t.cpu()\n",
        "        #         sub_heads_logits,sub_tails_logits,obj_heads_logits,obj_tails_logits=np.array(sub),np.array(tail),np.array(obj_h),np.array(obj_t)\n",
        "        #         inputs_id=inputs_id.cpu()\n",
        "        #         # for i in range(val_batch_size):\n",
        "        #         #     sub_heads_logits, sub_tails_logits, obj_heads_logits, obj_tails_logits=sub_heads_logits[i],sub_tails_logits[i],obj_heads_logits[i],obj_tails_logits[i]\n",
        "        #         #     input_ids=inputs_id[i]\n",
        "        #         #     triple_list=extrac_triple(sub_heads_logits,sub_tails_logits,obj_heads_logits,obj_tails_logits,input_ids)\n",
        "        #         #     dict_t[j]=triple_list\n",
        "        #         triple_list=extrac_triple(sub_heads_logits,sub_tails_logits,obj_heads_logits,obj_tails_logits,inputs_id)\n",
        "        #         dict_t[j]=triple_list\n",
        "        #     # print(dict_t)\n",
        "        #     json.dump(dict_t,f)\n",
        "        #     f.close()\n",
        "        #     print('-'*50)\n",
        "        #     print('val_train_epoch|{},val_sub_loss={},sub_h_acc={}，sub_t_acc={}'.format(epoch+1, sub_loss,sub_hacc/(len(val_data)*val_batch_size*128),\n",
        "        #                                                                         sub_tacc/(len(val_data)*val_batch_size*128)))\n",
        "        #     print('obj_loss={},obj_h_acc={}，obj_t_acc={}'.format(obj_loss,obj_hacc/(len(val_data)*val_batch_size*128*len(rel2id)),\n",
        "        #                                                                         obj_tacc/(len(val_data)*val_batch_size*128*len(rel2id))))\n",
        "        #     print('val_epoch{}|total_loss={}'.format(epoch+1,train_loss/vstep))\n",
        "        # precision, recall, f1_score = metric(val_data, exact_match=False, output_path='val_output.json')\n",
        "        # print(f'precision={precision},recall={recall},f1_score={f1_score}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9igx_ZvvBwoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VBQD-go5tWz0"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "# def train(net, train_set,val_set, num_epochs, learning_rate, batch_size):\n",
        "#     i = 1\n",
        "#     print('执行次数为：{}'.format(i))\n",
        "#     train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "#     t_total = len(train_loader) // num_epochs\n",
        "#     optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
        "#     # scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=t_total)\n",
        "\n",
        "#     net = net.to(device)\n",
        "#     net.train()\n",
        "#     # optimizer = AdamW(net.parameters(), lr=learning_rate)\n",
        "#     # criterier = nn.CrossEntropyLoss()\n",
        "#     # criterier=nn.MSELoss()\n",
        "#     for epoch in range(num_epochs):\n",
        "#         train_loss, sub_hacc,sub_tacc,obj_hacc,obj_tacc = 0, 0,0,0,0\n",
        "#         accum_iter = 4\n",
        "#         step = 1\n",
        "#         for batch_idx, data in enumerate(tqdm(train_loader)):\n",
        "#             inputs_id,att_mask,sub_htar,sub_ttar,obj_htar,obj_ttar = data\n",
        "#             text, mask, sub_htar,sub_ttar,obj_htar,obj_ttar =inputs_id.to(device),att_mask.to(device),sub_htar.to(device),sub_ttar.to(device),obj_htar.to(device),obj_ttar.to(device)\n",
        "#             optimizer.zero_grad()\n",
        "#             # with torch.set_grad_enabled(True):\n",
        "#             sub,tail,obj_h,obj_t, sub_loss,obj_loss,total_loss= net(text, mask, sub_htar,sub_ttar,obj_htar,obj_ttar)\n",
        "#             sub_htar,sub_ttar=sub_htar.unsqueeze(-1),sub_ttar.unsqueeze(-1)\n",
        "#             # label = label.float()\n",
        "#             # loss = criterier(pre, label)\n",
        "#             # obj_loss.backward()\n",
        "#             total_loss.backward()\n",
        "#             optimizer.step()\n",
        "#             # scheduler.step()\n",
        "#             # print(sub.ge(0.5).int())\n",
        "#             # print(subtar)\n",
        "#             sub_hacc+=(sub.ge(0.5).int()==sub_htar).sum().item()\n",
        "#             sub_tacc += (tail.ge(0.5).int() == sub_ttar).sum().item()\n",
        "#             obj_hacc+=(obj_h.ge(0.5).int()==obj_htar).sum().item()\n",
        "#             obj_tacc+=(obj_t.ge(0.5).int()==obj_ttar).sum().item()\n",
        "#             train_loss += total_loss.item()\n",
        "\n",
        "#             step += 1\n",
        "#             if step % 100 == 0:\n",
        "#                 print('train_epoch|{},sub_loss={},sub_h_acc={}，sub_t_acc={}'.format(epoch+1, sub_loss,sub_hacc/(step*batch_size*128),\n",
        "#                                                                                 sub_tacc/(step*batch_size*128)))\n",
        "#                 print('obj_loss={},obj_h_acc={}，obj_t_acc={}'.format(obj_loss,obj_hacc/(step*batch_size*128*len(rel2id)),\n",
        "#                                                                                 obj_tacc/(step*batch_size*128*len(rel2id))))\n",
        "#                 print('epoch{}|total_loss={}'.format(epoch+1,train_loss/step))\n",
        "#         torch.save(net.state_dict(), model_path)\n",
        "\n",
        "#         net.eval()\n",
        "#         val_loader = DataLoader(val_set,batch_size=1,shuffle=False)\n",
        "#         val_batch_size=1\n",
        "#         with torch.no_grad():\n",
        "#             dict_t = {}\n",
        "#             for j, data in enumerate(val_loader):\n",
        "#                 f=open('triple_list.json',mode='w+',encoding='utf_8')\n",
        "#                 train_loss, sub_hacc, sub_tacc, obj_hacc, obj_tacc = 0, 0, 0, 0, 0\n",
        "#                 inputs_id, att_mask, sub_htar, sub_ttar, obj_htar, obj_ttar = data\n",
        "#                 text, mask, sub_htar, sub_ttar, obj_htar, obj_ttar = inputs_id.to(device), att_mask.to(device), sub_htar.to(\n",
        "#                     device), sub_ttar.to(device), obj_htar.to(device), obj_ttar.to(device)\n",
        "#                 sub, tail, obj_h, obj_t, sub_loss, obj_loss, total_loss = model(text, mask, sub_htar, sub_ttar, obj_htar,                                                                    obj_ttar)\n",
        "#                 sub_h=sub.ge(0.5).int()\n",
        "#                 sub_t=tail.ge(0.5).int()\n",
        "#                 obj_h=obj_h.ge(0.5).int()\n",
        "#                 obj_t=obj_t.ge(0.5).int()\n",
        "#                 sub_hacc += (sub.ge(0.5).int() == sub_htar).sum().item()\n",
        "#                 sub_tacc += (tail.ge(0.5).int() == sub_ttar).sum().item()\n",
        "#                 obj_hacc += (obj_h.ge(0.5).int() == obj_htar).sum().item()\n",
        "#                 obj_tacc += (obj_t.ge(0.5).int() == obj_ttar).sum().item()\n",
        "#                 train_loss += total_loss.item()\n",
        "#                 # print('sub_h', sub_h)\n",
        "#                 # print('sub_t',sub_t)\n",
        "#                 # print('obj_h',obj_h)\n",
        "#                 # print('obj_t',obj_t)\n",
        "#                 # print(sub_hacc)\n",
        "#                 # print(sub_tacc)\n",
        "#                 # print(obj_hacc)\n",
        "#                 # print(obj_tacc)\n",
        "#                 sub, tail, obj_h, obj_t=sub.cpu(),tail.cpu(),obj_h.cpu(),obj_t.cpu()\n",
        "#                 sub_heads_logits,sub_tails_logits,obj_heads_logits,obj_tails_logits=np.array(sub),np.array(tail),np.array(obj_h),np.array(obj_t)\n",
        "#                 inputs_id=inputs_id.cpu()\n",
        "#                 def extrac_triple(sub_heads_logits,sub_tails_logits,obj_heads_logits,obj_tails_logits,input_ids):  # 抽取一个triple\n",
        "#                     h_bar=0.5\n",
        "#                     t_bar=0.5\n",
        "#                     sub_heads_logits = np.array(sub_heads_logits)\n",
        "#                     sub_tails_logits = np.array(sub_tails_logits)\n",
        "#                     tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "#                     sub_heads, sub_tails = np.where(sub_heads_logits > h_bar)[0], np.where(sub_tails_logits > t_bar)[0]\n",
        "#                     subjects = []\n",
        "#                     for sub_head in sub_heads:\n",
        "#                         sub_tail = sub_tails[sub_tails >= sub_head]\n",
        "#                         if len(sub_tail) > 0:\n",
        "#                             sub_tail = sub_tail[0]\n",
        "#                             if sub_tail==sub_head:\n",
        "#                                 subject = tokens[input_ids[sub_head]]\n",
        "#                                 subjects.append((subject, sub_head, sub_tail))\n",
        "#                             else:\n",
        "#                                 subject = tokens[input_ids[sub_head: sub_tail]]\n",
        "#                                 subjects.append((subject, sub_head, sub_tail))\n",
        "#                     if subjects:\n",
        "#                         triple_list = []\n",
        "#                         sub_heads, sub_tails = np.array([sub[1:] for sub in subjects]).T.reshape((2, -1, 1))\n",
        "#                         for i, subject in enumerate(subjects):\n",
        "#                             sub = subject[0]\n",
        "#                             sub = ''.join([i.lstrip(\"##\") for i in sub])\n",
        "#                             sub = ' '.join(sub.split('[unused1]'))\n",
        "#                             obj_heads, obj_tails = np.where(obj_heads_logits > h_bar), np.where(obj_tails_logits > t_bar)\n",
        "#                             for obj_head, rel_head in zip(*obj_heads):\n",
        "#                                 for obj_tail, rel_tail in zip(*obj_tails):\n",
        "#                                     if obj_head <= obj_tail and rel_head == rel_tail:\n",
        "#                                         rel = id2rel[rel_head]\n",
        "#                                         if obj_head==obj_tail:\n",
        "#                                             obj=tokens[input_ids[obj_head]]\n",
        "#                                         else:\n",
        "#                                             obj = tokens[input_ids[obj_head: obj_tail]]\n",
        "#                                         obj = ''.join([i.lstrip(\"##\") for i in obj])\n",
        "#                                         obj = ' '.join(obj.split('[unused1]'))\n",
        "#                                         triple_list.append((sub, rel, obj))\n",
        "#                                         break\n",
        "#                         triple_set = set()\n",
        "#                         for s, r, o in triple_list:\n",
        "#                             triple_set.add((s, r, o))\n",
        "#                         return list(triple_set)\n",
        "#                     else:\n",
        "#                         return []\n",
        "#                 for i in range(val_batch_size):\n",
        "#                     sub_heads_logits, sub_tails_logits, obj_heads_logits, obj_tails_logits=sub_heads_logits[i],sub_tails_logits[i],obj_heads_logits[i],obj_tails_logits[i]\n",
        "#                     input_ids=inputs_id[i]\n",
        "#                     triple_list=extrac_triple(sub_heads_logits,sub_tails_logits,obj_heads_logits,obj_tails_logits,input_ids)\n",
        "#                     dict_t[j]=triple_list\n",
        "#             # print(dict_t)\n",
        "#             json.dump(dict_t,f)\n",
        "#             f.close()\n",
        "#         precision, recall, f1_score = metric(val_data, exact_match=False, output_path='output.json')\n",
        "#         print(f'precision={precision},recall={recall},f1_score={f1_score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train_epoch"
      ],
      "metadata": {
        "id": "dBnkb3mRgm9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 文中模型训练"
      ],
      "metadata": {
        "id": "Tc0a7WVKBP_s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29LX1M3tfrXa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30b6c3a6-9f85-49e3-a5cb-a067816d7f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "执行次数为：1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 802/15584 [01:05<20:08, 12.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.039343155920505524,sub_h_acc=0.99696044921875，sub_t_acc=0.99761474609375\n",
            "obj_loss=0.06399550288915634,obj_h_acc=0.9995572916666666，obj_t_acc=0.9997037760416667\n",
            "epoch1|train_loss=71.43427553056972,total_loss=0.08929284441321216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1602/15584 [02:11<19:07, 12.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.06530480831861496,sub_h_acc=0.99699462890625，sub_t_acc=0.99758056640625\n",
            "obj_loss=0.06935432553291321,obj_h_acc=0.9995830281575521，obj_t_acc=0.9997177124023438\n",
            "epoch1|train_loss=145.77316790743498,total_loss=0.09110822994214686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 2402/15584 [03:16<17:57, 12.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.015951575711369514,sub_h_acc=0.997109375，sub_t_acc=0.997607421875\n",
            "obj_loss=0.05534094199538231,obj_h_acc=0.9996099175347222，obj_t_acc=0.9997227647569444\n",
            "epoch1|train_loss=213.1974768108048,total_loss=0.088832282004502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 3202/15584 [04:22<16:54, 12.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.0005129882483743131,sub_h_acc=0.997008056640625，sub_t_acc=0.997581787109375\n",
            "obj_loss=0.00258204760029912,obj_h_acc=0.999605458577474，obj_t_acc=0.9997221374511719\n",
            "epoch1|train_loss=286.7658813186572,total_loss=0.08961433791208037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 4002/15584 [05:27<15:59, 12.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.026952456682920456,sub_h_acc=0.99699365234375，sub_t_acc=0.99760595703125\n",
            "obj_loss=0.04722091555595398,obj_h_acc=0.9996066487630209，obj_t_acc=0.9997274983723958\n",
            "epoch1|train_loss=353.88088289472216,total_loss=0.08847022072368053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 4802/15584 [06:33<14:44, 12.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.17299556732177734,sub_h_acc=0.9969698079427083，sub_t_acc=0.9976021321614583\n",
            "obj_loss=0.2508247196674347,obj_h_acc=0.9996112738715278，obj_t_acc=0.9997311062282986\n",
            "epoch1|train_loss=420.32134558673715,total_loss=0.08756694699723691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 5602/15584 [07:39<13:39, 12.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.0010802315082401037,sub_h_acc=0.9969447544642858，sub_t_acc=0.9976402064732143\n",
            "obj_loss=0.04475496709346771,obj_h_acc=0.9996099853515625，obj_t_acc=0.9997315906343006\n",
            "epoch1|train_loss=491.8765814851213,total_loss=0.0878351038366288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 6402/15584 [08:44<12:35, 12.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.009528722614049911,sub_h_acc=0.9969143676757812，sub_t_acc=0.9976321411132812\n",
            "obj_loss=0.003369607264176011,obj_h_acc=0.9996122741699218，obj_t_acc=0.9997341156005859\n",
            "epoch1|train_loss=562.4309317299449,total_loss=0.08787983308280388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 7202/15584 [09:50<11:31, 12.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.04460179805755615,sub_h_acc=0.99685546875，sub_t_acc=0.9976356336805555\n",
            "obj_loss=0.035871587693691254,obj_h_acc=0.999610279224537，obj_t_acc=0.9997357177734375\n",
            "epoch1|train_loss=636.4132966666475,total_loss=0.0883907356481455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████▏    | 8002/15584 [10:55<10:24, 12.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.0006047900533303618,sub_h_acc=0.996903564453125，sub_t_acc=0.9976533203125\n",
            "obj_loss=0.014913925901055336,obj_h_acc=0.9996141967773438，obj_t_acc=0.9997389322916667\n",
            "epoch1|train_loss=705.6868214588103,total_loss=0.08821085268235129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▋    | 8802/15584 [12:01<09:17, 12.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.0040470086969435215,sub_h_acc=0.9969475763494318，sub_t_acc=0.9976808860085228\n",
            "obj_loss=0.015921585261821747,obj_h_acc=0.9996139433889678，obj_t_acc=0.9997404341264204\n",
            "epoch1|train_loss=779.1007997647284,total_loss=0.08853418179144641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 9602/15584 [13:06<08:13, 12.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=5.697547021554783e-05,sub_h_acc=0.9969600423177083，sub_t_acc=0.9976778157552083\n",
            "obj_loss=0.00015990171232260764,obj_h_acc=0.9996153598361545，obj_t_acc=0.999741465250651\n",
            "epoch1|train_loss=850.5764707550697,total_loss=0.0886017157036531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 10402/15584 [14:12<07:04, 12.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.020247969776391983,sub_h_acc=0.9970010141225961，sub_t_acc=0.99770263671875\n",
            "obj_loss=0.04385776072740555,obj_h_acc=0.9996192971254007，obj_t_acc=0.9997437149439102\n",
            "epoch1|train_loss=921.8450137902328,total_loss=0.08863894363367623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 11202/15584 [15:17<06:01, 12.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.010269559919834137,sub_h_acc=0.9970295061383928，sub_t_acc=0.997718505859375\n",
            "obj_loss=0.051650598645210266,obj_h_acc=0.9996196637834821，obj_t_acc=0.9997433326357887\n",
            "epoch1|train_loss=993.4648753024267,total_loss=0.08870222100914524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 77%|███████▋  | 12002/15584 [16:23<04:55, 12.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=2.606430098239798e-05,sub_h_acc=0.9970675455729167，sub_t_acc=0.9977190755208334\n",
            "obj_loss=1.5336900105467066e-05,obj_h_acc=0.999623779296875，obj_t_acc=0.999744615342882\n",
            "epoch1|train_loss=1064.1892497704685,total_loss=0.08868243748087237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 12802/15584 [17:29<03:48, 12.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.0044414810836315155,sub_h_acc=0.997080078125，sub_t_acc=0.9977107238769531\n",
            "obj_loss=0.027900777757167816,obj_h_acc=0.9996263885498047，obj_t_acc=0.9997452290852865\n",
            "epoch1|train_loss=1139.4066093578658,total_loss=0.08901614135608327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 13602/15584 [18:34<02:43, 12.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.0016903569921851158,sub_h_acc=0.9971057846966912，sub_t_acc=0.9977023494944853\n",
            "obj_loss=0.015968888998031616,obj_h_acc=0.999627565870098，obj_t_acc=0.9997446576286765\n",
            "epoch1|train_loss=1216.726150203407,total_loss=0.0894651581031917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 14402/15584 [19:40<01:36, 12.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.0005905277794227004,sub_h_acc=0.997125244140625，sub_t_acc=0.9976987033420139\n",
            "obj_loss=0.04364532232284546,obj_h_acc=0.9996311442057292，obj_t_acc=0.9997463424117476\n",
            "epoch1|train_loss=1285.5508386289312,total_loss=0.08927436379367577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 15202/15584 [20:45<00:31, 12.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|1,sub_loss=0.008302723988890648,sub_h_acc=0.9971321186266447，sub_t_acc=0.9977040501644737\n",
            "obj_loss=0.13331863284111023,obj_h_acc=0.9996320757949562，obj_t_acc=0.9997471002946821\n",
            "epoch1|train_loss=1360.0363155761897,total_loss=0.08947607339317037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15584/15584 [21:16<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------\n",
            "val_train_epoch|1,val_sub_loss=5.405672709457576e-06,sub_h_acc=125.70866613418531，sub_t_acc=125.80880591054313\n",
            "obj_loss=1.894318120321259e-05,obj_h_acc=0.999621439030884，obj_t_acc=0.9997940794728435\n",
            "val_epoch1|total_loss=0.08710413585104891\n",
            "correct_num:176.0000000001\n",
            "predict_num:497.0000000001\n",
            "gold_num:390.0000000001\n",
            "precision=0.3541247484910756,recall=0.45128205128219195,f1_score=0.3968432919956264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 802/15584 [01:05<20:06, 12.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|2,sub_loss=0.05906885862350464,sub_h_acc=0.9976513671875，sub_t_acc=0.99842041015625\n",
            "obj_loss=0.001001895871013403,obj_h_acc=0.9996732584635417，obj_t_acc=0.9998264567057291\n",
            "epoch2|train_loss=54.656187960225,total_loss=0.06832023495028125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1602/15584 [02:10<18:57, 12.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|2,sub_loss=0.00011282834748271853,sub_h_acc=0.9976611328125，sub_t_acc=0.9983935546875\n",
            "obj_loss=4.390787216834724e-05,obj_h_acc=0.9995419311523438，obj_t_acc=0.9996854654947916\n",
            "epoch2|train_loss=179.06074381446797,total_loss=0.11191296488404248\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 2402/15584 [03:15<17:52, 12.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|2,sub_loss=1.929621021190542e-06,sub_h_acc=0.9978426106770834，sub_t_acc=0.9984415690104167\n",
            "obj_loss=0.11820939928293228,obj_h_acc=0.9996153428819444，obj_t_acc=0.9997391086154513\n",
            "epoch2|train_loss=235.00741282325123,total_loss=0.09791975534302134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 3202/15584 [04:20<16:51, 12.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|2,sub_loss=0.00012784528371412307,sub_h_acc=0.997943115234375，sub_t_acc=0.9984454345703125\n",
            "obj_loss=0.016591859981417656,obj_h_acc=0.999643300374349，obj_t_acc=0.9997656758626302\n",
            "epoch2|train_loss=297.18694411733577,total_loss=0.09287092003666743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 4002/15584 [05:25<15:46, 12.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|2,sub_loss=0.0231076180934906,sub_h_acc=0.9980439453125，sub_t_acc=0.99852099609375\n",
            "obj_loss=0.015330925583839417,obj_h_acc=0.9996640625，obj_t_acc=0.9997810872395834\n",
            "epoch2|train_loss=354.8754397269197,total_loss=0.08871885993172993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 4802/15584 [06:30<14:46, 12.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|2,sub_loss=5.4546114824916e-07,sub_h_acc=0.9980802408854167，sub_t_acc=0.998531494140625\n",
            "obj_loss=4.428205784279271e-07,obj_h_acc=0.9996758015950521，obj_t_acc=0.9997904798719618\n",
            "epoch2|train_loss=414.2302943135103,total_loss=0.08629797798198131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 5602/15584 [07:36<13:37, 12.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_epoch|2,sub_loss=6.143033260741504e-06,sub_h_acc=0.9981044224330358，sub_t_acc=0.9985484095982143\n",
            "obj_loss=0.08743640780448914,obj_h_acc=0.9996658470517114，obj_t_acc=0.9997771344866071\n",
            "epoch2|train_loss=478.0987109346366,total_loss=0.08537476980975653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 6200/15584 [08:24<12:44, 12.28it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-d4b519b57f9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-ef7ef270f837>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_set, val_set, num_epochs, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msub_htar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub_ttar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msub_htar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msub_ttar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0msub_hacc\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0msub_htar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0msub_tacc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msub_ttar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "EPOCHS =30\n",
        "train(model, train_set,dev_set, EPOCHS, learning_rate=1e-5, batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS =30\n",
        "train(model, train_set,dev_set, EPOCHS, learning_rate=1e-5, batch_size=2)"
      ],
      "metadata": {
        "id": "mIG6gd-SFFMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS =10\n",
        "train(model, train_set,dev_set, EPOCHS, learning_rate=1e-5, batch_size=16)"
      ],
      "metadata": {
        "id": "cRuOq4rDP1rB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS =10\n",
        "train(model, train_set,dev_set, EPOCHS, learning_rate=1e-5, batch_size=16)"
      ],
      "metadata": {
        "id": "QkPNcvvZUaJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_set)"
      ],
      "metadata": {
        "id": "nLamIUxT_uvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "val_loader = DataLoader(dev_set,batch_size=1,shuffle=False)\n",
        "val_batch_size=1\n",
        "with torch.no_grad():\n",
        "    dict_t = {}\n",
        "    vstep = 0\n",
        "    train_loss, sub_hacc, sub_tacc, obj_hacc, obj_tacc = 0, 0, 0, 0, 0\n",
        "    f = open('val_traintriple_list.json', mode='w+', encoding='utf_8')\n",
        "  \n",
        "    for j, data in enumerate(val_loader):\n",
        "        inputs_id, att_mask, sub_htar, sub_ttar, obj_htar, obj_ttar = data\n",
        "        text, mask, sub_htar, sub_ttar, obj_htar, obj_ttar = inputs_id.to(device), att_mask.to(device), sub_htar.to(\n",
        "            device), sub_ttar.to(device), obj_htar.to(device), obj_ttar.to(device)\n",
        "        sub, tail, obj_h, obj_t, sub_loss, obj_loss, total_loss = model(text, mask, sub_htar, sub_ttar, obj_htar,\n",
        "                                                                        obj_ttar)\n",
        "        sub_h = sub.ge(0.5).int()\n",
        "        sub_t = tail.ge(0.5).int()\n",
        "        obj_h = obj_h.ge(0.5).int()\n",
        "        obj_t = obj_t.ge(0.5).int()\n",
        "        sub_hacc += (sub.ge(0.5).int() == sub_htar).sum().item()\n",
        "        sub_tacc += (tail.ge(0.5).int() == sub_ttar).sum().item()\n",
        "        obj_hacc += (obj_h.ge(0.5).int() == obj_htar).sum().item()\n",
        "        obj_tacc += (obj_t.ge(0.5).int() == obj_ttar).sum().item()\n",
        "        train_loss += total_loss.item()\n",
        "        vstep += 1\n",
        "        sub, tail, obj_h, obj_t = sub.cpu(), tail.cpu(), obj_h.cpu(), obj_t.cpu()\n",
        "        sub_heads_logits, sub_tails_logits, obj_heads_logits, obj_tails_logits = np.array(sub), np.array(\n",
        "            tail), np.array(obj_h), np.array(obj_t)\n",
        "        inputs_id = inputs_id.cpu()\n",
        "        # for i in range(val_batch_size):\n",
        "        #     sub_heads_logits, sub_tails_logits, obj_heads_logits, obj_tails_logits=sub_heads_logits[i],sub_tails_logits[i],obj_heads_logits[i],obj_tails_logits[i]\n",
        "        #     input_ids=inputs_id[i]\n",
        "        #     triple_list=extrac_triple(sub_heads_logits,sub_tails_logits,obj_heads_logits,obj_tails_logits,input_ids)\n",
        "        #     dict_t[j]=triple_list\n",
        "        triple_list = extrac_triple(sub_heads_logits[0], sub_tails_logits[0], obj_heads_logits[0], obj_tails_logits[0], inputs_id[0])\n",
        "        dict_t[j] = triple_list\n",
        "        if vstep<20:\n",
        "          tokens = tokenizer.convert_ids_to_tokens(inputs_id[0])\n",
        "          print(tokens)\n",
        "          print(sub_h,sub_t)\n",
        "          print('\\n')\n",
        "          print(triple_list)\n",
        "    # print(dict_t)\n",
        "    json.dump(dict_t, f)\n",
        "    f.close()\n",
        "    print('-' * 50)\n",
        "    print('val_train_epoch|{},val_sub_loss={},sub_h_acc={}，sub_t_acc={}'.format( 1, sub_loss, sub_hacc / (\n",
        "                len(dev_set) * val_batch_size * 128),\n",
        "                                                                                sub_tacc / (\n",
        "                                                                                            len(dev_set) * val_batch_size * 128)))\n",
        "    print('obj_loss={},obj_h_acc={}，obj_t_acc={}'.format(obj_loss, obj_hacc / (\n",
        "                len(dev_set) * val_batch_size * 128 * len(rel2id)),\n",
        "                                                         obj_tacc / (len(dev_set) * val_batch_size * 128 * len(\n",
        "                                                             rel2id))))\n",
        "    print('val_epoch{}|total_loss={}'.format( 1, train_loss / vstep))\n",
        "precision, recall, f1_score = metric(val_data, exact_match=False, output_path='val_output.json')\n",
        "print(f'precision={precision},recall={recall},f1_score={f1_score}')"
      ],
      "metadata": {
        "id": "v6dou2MQyLT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dev_set)"
      ],
      "metadata": {
        "id": "PKDOMuJr-ZQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dev_set)"
      ],
      "metadata": {
        "id": "aw1fgQyM4yDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_data)"
      ],
      "metadata": {
        "id": "joyaQBRwoDDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0 \n",
        "for i in range(5):\n",
        "  i+=1\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "pIh46TcbnJwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOes2QgagSno"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PgBzuQdgFCc"
      },
      "outputs": [],
      "source": [
        "BERT_MAX_LEN = 128\n",
        "RANDOM_SEED = 2019\n",
        "model_path='./model.ckpt'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrY9zuG0gfXP"
      },
      "source": [
        "test数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfcj5n9Kge6K"
      },
      "outputs": [],
      "source": [
        "def to_tuple(sent):\n",
        "    triple_list = []\n",
        "    for triple in sent['triple_list']:\n",
        "        triple_list.append(tuple(triple))\n",
        "    sent['triple_list'] = triple_list\n",
        "def load_data(test_path, rel_dict_path):\n",
        "    test_data = json.load(open(test_path))\n",
        "    id2rel, rel2id = json.load(open(rel_dict_path))\n",
        "\n",
        "    id2rel = {int(i): j for i, j in id2rel.items()}\n",
        "    num_rels = len(id2rel)\n",
        "\n",
        "    for sent in test_data:\n",
        "        to_tuple(sent)\n",
        "\n",
        "    print(\"test_data len:\", len(test_data))\n",
        "\n",
        "    return test_data, id2rel, rel2id, num_rels\n",
        "\n",
        "def find_head_idx(source, target):\n",
        "    target_len = len(target)\n",
        "    for i in range(len(source)):\n",
        "        if source[i: i + target_len] == target:\n",
        "            return i\n",
        "    return -1\n",
        "class data_generator:\n",
        "    def __init__(self, data, tokenizer, rel2id, num_rels, maxlen):\n",
        "        self.data = data\n",
        "        self.batch_size = len(self.data)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.maxlen = maxlen\n",
        "        self.rel2id = rel2id\n",
        "        self.num_rels = num_rels\n",
        "    def __len__(self):\n",
        "        return self.batch_size\n",
        "    def generator(self):\n",
        "        while True:\n",
        "            idxs = list(range(len(self.data)))\n",
        "            # np.random.seed(RANDOM_SEED)\n",
        "            # np.random.shuffle(idxs)\n",
        "            tokens_batch, segments_batch, sub_heads_batch, sub_tails_batch, sub_head_batch, sub_tail_batch, obj_heads_batch, obj_tails_batch = [], [], [], [], [], [], [], []\n",
        "            for idx in idxs:\n",
        "                line = self.data[idx]\n",
        "                text = ' '.join(line['text'].split()[:self.maxlen])\n",
        "                tokens = self.tokenizer.tokenize(text)\n",
        "                if len(tokens) > BERT_MAX_LEN:\n",
        "                    tokens = tokens[:BERT_MAX_LEN]\n",
        "\n",
        "\n",
        "                s2ro_map = {}\n",
        "                for triple in line['triple_list']:\n",
        "                    triple = (self.tokenizer.tokenize(triple[0]), triple[1], self.tokenizer.tokenize(triple[2]))\n",
        "                    sub_head_idx = find_head_idx(tokens, triple[0])\n",
        "                    obj_head_idx = find_head_idx(tokens, triple[2])\n",
        "                    if sub_head_idx != 0 and obj_head_idx != 0:\n",
        "                        sub = (sub_head_idx, sub_head_idx + len(triple[0]) - 1)\n",
        "                        if sub not in s2ro_map:\n",
        "                            s2ro_map[sub] = []\n",
        "                        s2ro_map[sub].append((obj_head_idx,##subject to relation object\n",
        "                                           obj_head_idx + len(triple[2]) - 1,#obj_tail_idx\n",
        "                                           self.rel2id[triple[1]]))#rel\n",
        "                text_len = BERT_MAX_LEN\n",
        "                if s2ro_map:\n",
        "                    # token_ids, segment_ids = self.tokenizer.encode(text)\n",
        "                    inputs=self.tokenizer(text,return_tensors='pt',add_special_tokens=False,truncation=True,padding=True,max_length=BERT_MAX_LEN)\n",
        "                    token_ids, segment_ids=inputs['input_ids'],inputs['attention_mask']\n",
        "                    pad_len=BERT_MAX_LEN-token_ids.shape[1]\n",
        "                    pad_seq=torch.zeros(1,pad_len)\n",
        "                    token_ids=torch.cat((token_ids,pad_seq),dim=1)\n",
        "                    segment_ids=torch.cat((segment_ids,pad_seq),dim=1)\n",
        "                    # if inputs.shape[1]<BERT_MAX_LEN:\n",
        "                    #     inputs=inputs\n",
        "                    #     [seq, [padding] * (max_length - len(seq))]\n",
        "\n",
        "                    # if len(token_ids) > text_len:\n",
        "                    #     token_ids = token_ids[:text_len]\n",
        "                    #     segment_ids = segment_ids[:text_len]\n",
        "                    tokens_batch.append(token_ids)\n",
        "                    segments_batch.append(segment_ids)\n",
        "                    sub_heads, sub_tails=torch.zeros(text_len),torch.zeros(text_len)\n",
        "                    for s in s2ro_map:\n",
        "                        sub_heads[s[0]] = 1\n",
        "                        sub_tails[s[1]] = 1\n",
        "                    sub_head, sub_tail = choice(list(s2ro_map.keys()))\n",
        "                    obj_heads, obj_tails = torch.zeros((text_len, self.num_rels)), torch.zeros((text_len, self.num_rels))\n",
        "                    for ro in s2ro_map.get((sub_head, sub_tail), []):\n",
        "                        obj_heads[ro[0]][ro[2]] = 1\n",
        "                        obj_tails[ro[1]][ro[2]] = 1\n",
        "                    sub_heads_batch.append(sub_heads)\n",
        "                    sub_tails_batch.append(sub_tails)\n",
        "                    sub_head_batch.append([sub_head])\n",
        "                    sub_tail_batch.append([sub_tail])\n",
        "                    obj_heads_batch.append(obj_heads)\n",
        "                    obj_tails_batch.append(obj_tails)\n",
        "            return tokens_batch, segments_batch, sub_heads_batch, sub_tails_batch, sub_head_batch, sub_tail_batch, obj_heads_batch, obj_tails_batch\n",
        "# from changshi2 import data_generator\n",
        "\n",
        "test_data, id2rel, rel2id, num_rels = load_data('./test_triples.json', rel_dict_path='./rel2id.json')\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "maxlen = 100\n",
        "test_tokens_batch, segments_batch, sub_heads_batch, sub_tails_batch, sub_head_batch, sub_tail_batch, obj_heads_batch, obj_tails_batch=data_generator(test_data, tokenizer, rel2id, num_rels, maxlen)\\\n",
        "        .generator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1AQt9YtsWbt"
      },
      "outputs": [],
      "source": [
        "class data_generator:\n",
        "    def __init__(self, data, tokenizer, rel2id, num_rels, maxlen):\n",
        "        self.data = data\n",
        "        self.batch_size = len(self.data)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.maxlen = maxlen\n",
        "        self.rel2id = rel2id\n",
        "        self.num_rels = num_rels\n",
        "    def __len__(self):\n",
        "        return self.batch_size\n",
        "    def generator(self):\n",
        "        # while True:\n",
        "            idxs = list(range(len(self.data)))\n",
        "            # np.random.seed(RANDOM_SEED)\n",
        "            # np.random.shuffle(idxs)\n",
        "            tokens_batch, segments_batch, sub_heads_batch, sub_tails_batch, sub_head_batch, sub_tail_batch, obj_heads_batch, obj_tails_batch = [], [], [], [], [], [], [], []\n",
        "            for idx in idxs:\n",
        "                line = self.data[idx]\n",
        "                text = ' '.join(line['text'].split()[:self.maxlen])\n",
        "                tokens = self.tokenizer.tokenize(text)\n",
        "                if len(tokens) > BERT_MAX_LEN:\n",
        "                    tokens = tokens[:BERT_MAX_LEN]\n",
        "\n",
        "\n",
        "                s2ro_map = {}\n",
        "                for triple in line['triple_list']:\n",
        "                    triple = (self.tokenizer.tokenize(triple[0]), triple[1], self.tokenizer.tokenize(triple[2]))\n",
        "                    sub_head_idx = find_head_idx(tokens, triple[0])\n",
        "                    obj_head_idx = find_head_idx(tokens, triple[2])\n",
        "                    if sub_head_idx != 0 and obj_head_idx != 0:\n",
        "                        sub = (sub_head_idx, sub_head_idx + len(triple[0]) - 1)\n",
        "                        if sub not in s2ro_map:\n",
        "                            s2ro_map[sub] = []\n",
        "                        s2ro_map[sub].append((obj_head_idx,##subject to relation object\n",
        "                                           obj_head_idx + len(triple[2]) - 1,#obj_tail_idx\n",
        "                                           self.rel2id[triple[1]]))#rel\n",
        "                text_len = BERT_MAX_LEN\n",
        "                inputs=self.tokenizer(text,return_tensors='pt',add_special_tokens=False,truncation=True,padding=True,max_length=BERT_MAX_LEN)\n",
        "                token_ids, segment_ids=inputs['input_ids'],inputs['attention_mask']\n",
        "                pad_len=BERT_MAX_LEN-token_ids.shape[1]\n",
        "                pad_seq=torch.zeros(1,pad_len)\n",
        "                token_ids=torch.cat((token_ids,pad_seq),dim=1)\n",
        "                segment_ids=torch.cat((segment_ids,pad_seq),dim=1)\n",
        "                    # if inputs.shape[1]<BERT_MAX_LEN:\n",
        "                    #     inputs=inputs\n",
        "                    #     [seq, [padding] * (max_length - len(seq))]\n",
        "\n",
        "                    # if len(token_ids) > text_len:\n",
        "                    #     token_ids = token_ids[:text_len]\n",
        "                    #     segment_ids = segment_ids[:text_len]\n",
        "                tokens_batch.append(token_ids)\n",
        "                segments_batch.append(segment_ids)\n",
        "                sub_heads, sub_tails=torch.zeros(text_len),torch.zeros(text_len)\n",
        "                obj_heads, obj_tails = torch.zeros((text_len, self.num_rels)), torch.zeros((text_len, self.num_rels))\n",
        "                if s2ro_map:\n",
        "                    # token_ids, segment_ids = self.tokenizer.encode(text)\n",
        "                    \n",
        "                    \n",
        "                    for s in s2ro_map:\n",
        "                        sub_heads[s[0]] = 1\n",
        "                        sub_tails[s[1]] = 1\n",
        "                    # sub_head, sub_tail = choice(list(s2ro_map.keys()))\n",
        "\n",
        "                    \n",
        "                    sub=list(s2ro_map.keys())\n",
        "                    for sub_head,sub_tail in sub:\n",
        "                        for ro in s2ro_map.get((sub_head, sub_tail), []):\n",
        "                            obj_heads[ro[0]][ro[2]] = 1\n",
        "                            obj_tails[ro[1]][ro[2]] = 1\n",
        "                        # sub_head_batch.append([sub_head])\n",
        "                        # sub_tail_batch.append([sub_tail])\n",
        "                    # print(sub_heads,sub_tails,torch.where(obj_heads==1),torch.where(obj_tails)==1)\n",
        "                sub_heads_batch.append(sub_heads)\n",
        "                sub_tails_batch.append(sub_tails)\n",
        "                obj_heads_batch.append(obj_heads)\n",
        "                obj_tails_batch.append(obj_tails)\n",
        "            return tokens_batch, segments_batch, sub_heads_batch, sub_tails_batch, sub_head_batch, sub_tail_batch, obj_heads_batch, obj_tails_batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data, id2rel, rel2id, num_rels = load_data('./test_triples.json', rel_dict_path='./rel2id.json')\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "maxlen = 100\n",
        "test_tokens_batch, segments_batch, sub_heads_batch, sub_tails_batch, sub_head_batch, sub_tail_batch, obj_heads_batch, obj_tails_batch=data_generator(test_data, tokenizer, rel2id, num_rels, maxlen)\\\n",
        "        .generator()"
      ],
      "metadata": {
        "id": "82GX7dc-sb2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7wGczIUgl3f"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokens_batch=torch.cat([l for l in test_tokens_batch]).int()\n",
        "print(tokens_batch.shape)\n",
        "segments_batch=torch.cat([l for l in segments_batch]).int()#sents_length*128\n",
        "sub_heads_batch=torch.cat([l for l in sub_heads_batch]).reshape(tokens_batch.shape[0],-1).float()#128\n",
        "sub_tails_batch=torch.cat([l for l in sub_tails_batch]).reshape(tokens_batch.shape[0],-1).float()\n",
        "obj_heads_batch=torch.cat([l for l in obj_heads_batch]).reshape(tokens_batch.shape[0],128,-1).float()\n",
        "obj_tails_batch=torch.cat([l for l in obj_tails_batch]).reshape(tokens_batch.shape[0],128,-1).float()\n",
        "print(type(segments_batch))\n",
        "print(segments_batch.shape)\n",
        "print(obj_heads_batch.shape)\n",
        "print(type(obj_heads_batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8teiesPguTr"
      },
      "outputs": [],
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'gpu'\n",
        "model=E2EModel().to(device)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "optim=torch.optim.Adam(model.parameters(),lr=1e-5)\n",
        "from torch.utils.data import DataLoader,Dataset,TensorDataset\n",
        "train_set=TensorDataset(tokens_batch, segments_batch, sub_heads_batch, sub_tails_batch,obj_heads_batch,obj_tails_batch)\n",
        "def partial_match(pred_set, gold_set):\n",
        "    pred = {(i[0].split(' ')[0] if len(i[0].split(' ')) > 0 else i[0], i[1],\n",
        "                 i[2].split(' ')[0] if len(i[2].split(' ')) > 0 else i[2]) for i in pred_set}\n",
        "    gold = {(i[0].split(' ')[0] if len(i[0].split(' ')) > 0 else i[0], i[1],\n",
        "                 i[2].split(' ')[0] if len(i[2].split(' ')) > 0 else i[2]) for i in gold_set}\n",
        "    return pred, gold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk0UV3S4hWp-"
      },
      "source": [
        "### batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v34QXiprhUOE"
      },
      "outputs": [],
      "source": [
        "batch_size=1\n",
        "test_loader = DataLoader(train_set,batch_size=batch_size,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test测试版"
      ],
      "metadata": {
        "id": "W2w93cWzfjF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_batch_size = 1\n",
        "with torch.no_grad():\n",
        "    dict_t = {}\n",
        "    vstep = 0\n",
        "    train_loss, sub_hacc, sub_tacc, obj_hacc, obj_tacc = 0, 0, 0, 0, 0\n",
        "    for j, data in enumerate(test_loader):\n",
        "        f = open('triple_list_test.json', mode='w+', encoding='utf_8')\n",
        "\n",
        "        inputs_id, att_mask, sub_htar, sub_ttar, obj_htar, obj_ttar = data\n",
        "        text, mask, sub_htar, sub_ttar, obj_htar, obj_ttar = inputs_id.to(device), att_mask.to(device), sub_htar.to(\n",
        "            device), sub_ttar.to(device), obj_htar.to(device), obj_ttar.to(device)\n",
        "        sub, tail, obj_h, obj_t, sub_loss, obj_loss, total_loss = model(text, mask, sub_htar, sub_ttar, obj_htar,\n",
        "                                                                        obj_ttar)\n",
        "        sub_h = sub.ge(0.5).int()\n",
        "        sub_t = tail.ge(0.5).int()\n",
        "        obj_h = obj_h.ge(0.5).int()\n",
        "        obj_t = obj_t.ge(0.5).int()\n",
        "        sub_hacc += (sub.ge(0.5).int() == sub_htar).sum().item()\n",
        "        sub_tacc += (tail.ge(0.5).int() == sub_ttar).sum().item()\n",
        "        obj_hacc += (obj_h.ge(0.5).int() == obj_htar).sum().item()\n",
        "        obj_tacc += (obj_t.ge(0.5).int() == obj_ttar).sum().item()\n",
        "        train_loss += total_loss.item()\n",
        "        # print('sub_h', sub_h)\n",
        "        # print('sub_t',sub_t)\n",
        "        # print('obj_h',obj_h)\n",
        "        # print('obj_t',obj_t)\n",
        "        # print(sub_hacc)\n",
        "        # print(sub_tacc)\n",
        "        # print(obj_hacc)\n",
        "        # print(obj_tacc)\n",
        "        vstep += 1\n",
        "        sub, tail, obj_h, obj_t = sub.cpu(), tail.cpu(), obj_h.cpu(), obj_t.cpu()\n",
        "        sub_heads_logits, sub_tails_logits, obj_heads_logits, obj_tails_logits = np.array(sub), np.array(\n",
        "            tail), np.array(obj_h), np.array(obj_t)\n",
        "        inputs_id = inputs_id.cpu()\n",
        "\n",
        "\n",
        "        def extrac_triple(sub_heads_logits, sub_tails_logits, obj_heads_logits, obj_tails_logits,\n",
        "                          input_ids):  # 抽取一个triple\n",
        "            h_bar = 0.5\n",
        "            t_bar = 0.5\n",
        "            sub_heads_logits = np.array(sub_heads_logits)\n",
        "            sub_tails_logits = np.array(sub_tails_logits)\n",
        "            tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "            # print(tokens)\n",
        "            sub_heads, sub_tails = np.where(sub_heads_logits > h_bar)[0], np.where(sub_tails_logits > t_bar)[0]\n",
        "            subjects = []\n",
        "            for sub_head in sub_heads:\n",
        "                sub_tail = sub_tails[sub_tails >= sub_head]\n",
        "                if len(sub_tail) > 0:\n",
        "                    sub_tail = sub_tail[0]\n",
        "                    if sub_tail == sub_head:\n",
        "                        subject = tokens[sub_head]\n",
        "                        subjects.append((subject, sub_head, sub_tail))\n",
        "                    else:\n",
        "                        subject = tokens[sub_head: sub_tail]\n",
        "                        subjects.append((subject, sub_head, sub_tail))\n",
        "            if subjects:\n",
        "                triple_list = []\n",
        "                sub_heads, sub_tails = np.array([sub[1:] for sub in subjects]).T.reshape((2, -1, 1))\n",
        "                for i, subject in enumerate(subjects):\n",
        "                    sub = subject[0]\n",
        "                    sub = ''.join([i.lstrip(\"##\") for i in sub])\n",
        "                    sub = ' '.join(sub.split('[unused1]'))\n",
        "                    obj_heads, obj_tails = np.where(obj_heads_logits > h_bar), np.where(obj_tails_logits > t_bar)\n",
        "                    for obj_head, rel_head in zip(*obj_heads):\n",
        "                        for obj_tail, rel_tail in zip(*obj_tails):\n",
        "                            if obj_head <= obj_tail and rel_head == rel_tail:\n",
        "                                rel = id2rel[rel_head]\n",
        "                                if obj_head == obj_tail:\n",
        "                                    obj = tokens[obj_head]\n",
        "                                else:\n",
        "                                    obj = tokens[obj_head: obj_tail]\n",
        "                                obj = ''.join([i.lstrip(\"##\") for i in obj])\n",
        "                                obj = ' '.join(obj.split('[unused1]'))\n",
        "                                triple_list.append((sub, rel, obj))\n",
        "                                break\n",
        "                triple_set = set()\n",
        "                for s, r, o in triple_list:\n",
        "                    triple_set.add((s, r, o))\n",
        "                return list(triple_set)\n",
        "            else:\n",
        "                return []\n",
        "\n",
        "\n",
        "        for i in range(val_batch_size):\n",
        "            sub_heads_logits, sub_tails_logits, obj_heads_logits, obj_tails_logits = sub_heads_logits[i], \\\n",
        "                                                                                     sub_tails_logits[i], \\\n",
        "                                                                                     obj_heads_logits[i], \\\n",
        "                                                                                     obj_tails_logits[i]\n",
        "            input_ids = inputs_id[i]\n",
        "            triple_list = extrac_triple(sub_heads_logits, sub_tails_logits, obj_heads_logits, obj_tails_logits,\n",
        "                                        input_ids)\n",
        "            dict_t[j] = triple_list\n",
        "\n",
        "    # print(dict_t)\n",
        "    json.dump(dict_t, f)\n",
        "    f.close()\n",
        "    print('-' * 50)\n",
        "    print('val_train_epoch|{},val_sub_loss={},sub_h_acc={}，sub_t_acc={}'.format( 1, sub_loss, sub_hacc / (\n",
        "                vstep * val_batch_size * 128),\n",
        "                                                                                sub_tacc / (\n",
        "                                                                                            vstep * val_batch_size * 128)))\n",
        "    print('obj_loss={},obj_h_acc={}，obj_t_acc={}'.format(obj_loss,obj_hacc / (vstep * val_batch_size * 128 * len(rel2id)),obj_tacc / (vstep * val_batch_size * 128 * len(rel2id))))\n",
        "    print('val_epoch{}|total_loss={}'.format(1, train_loss / vstep))\n"
      ],
      "metadata": {
        "id": "N5-nNG50fatA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S96iWrOjuP7g"
      },
      "outputs": [],
      "source": [
        "def metric(eval_data,exact_match=False, output_path=None):\n",
        "    if output_path:\n",
        "        F = open(output_path, 'w+')\n",
        "    orders = ['subject', 'relation', 'object']\n",
        "    correct_num, predict_num, gold_num = 1e-10, 1e-10, 1e-10\n",
        "    # for line in tqdm(iter(eval_data)):\n",
        "    with open('triple_list_test.json') as f:\n",
        "        f=f.readline()\n",
        "        dict_=json.loads(f)\n",
        "    for i,line in enumerate(eval_data):\n",
        "        if i<tokens_batch.shape[0]:\n",
        "            pre=dict_[str(i)]\n",
        "            Pred_triples=set([tuple(l) for l in pre])\n",
        "            Gold_triples = set(tuple(l) for l in line['triple_list'])\n",
        "            Pred_triples_eval, Gold_triples_eval = partial_match(Pred_triples, Gold_triples) if not exact_match else (Pred_triples, Gold_triples)\n",
        "\n",
        "            correct_num += len(Pred_triples_eval & Gold_triples_eval)\n",
        "            predict_num += len(Pred_triples_eval)\n",
        "            gold_num += len(Gold_triples_eval)\n",
        "\n",
        "            if output_path:\n",
        "                result = json.dumps({\n",
        "                    'text': line['text'],\n",
        "                    'triple_list_gold': [\n",
        "                        dict(zip(orders, triple)) for triple in Gold_triples\n",
        "                    ],\n",
        "                    'triple_list_pred': [\n",
        "                        dict(zip(orders, triple)) for triple in Pred_triples\n",
        "                    ],\n",
        "                    'new': [\n",
        "                        dict(zip(orders, triple)) for triple in Pred_triples - Gold_triples\n",
        "                    ],\n",
        "                    'lack': [\n",
        "                        dict(zip(orders, triple)) for triple in Gold_triples - Pred_triples\n",
        "                    ]\n",
        "                }, ensure_ascii=False, indent=4)\n",
        "                F.write(result + '\\n')\n",
        "\n",
        "\n",
        "    precision = correct_num / predict_num\n",
        "    recall = correct_num / gold_num\n",
        "    f1_score = 2 * precision * recall / (precision + recall)\n",
        "\n",
        "    print(f'correct_num:{correct_num}\\npredict_num:{predict_num}\\ngold_num:{gold_num}')\n",
        "    return precision, recall, f1_score\n",
        "\n",
        "precision, recall, f1_score=metric(test_data,exact_match=False, output_path='output.json')\n",
        "print(precision, recall, f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1_score = metric(test_data, exact_match=False, output_path='output_test.json')\n",
        "print(f'precision={precision},recall={recall},f1_score={f1_score}')"
      ],
      "metadata": {
        "id": "zX3ZIqxihEsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ31Z4pTgxpD"
      },
      "source": [
        "### test_pro"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "h_q09bajeGHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy5ZzDF6gv6e"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    dict_t = {}\n",
        "    train_loss, sub_hacc, sub_tacc, obj_hacc, obj_tacc = 0, 0, 0, 0, 0\n",
        "    for j, data in enumerate(test_loader):\n",
        "        f=open('triple_list.json',mode='w+',encoding='utf_8')\n",
        "        \n",
        "        inputs_id, att_mask, sub_htar, sub_ttar, obj_htar, obj_ttar = data\n",
        "        text, mask, sub_htar, sub_ttar, obj_htar, obj_ttar = inputs_id.to(device), att_mask.to(device), sub_htar.to(\n",
        "            device), sub_ttar.to(device), obj_htar.to(device), obj_ttar.to(device)\n",
        "        sub, tail, obj_h, obj_t, sub_loss, obj_loss, total_loss = model(text, mask, sub_htar, sub_ttar, obj_htar,                                                                    obj_ttar)\n",
        "        sub_h=sub.ge(0.5).int()\n",
        "        sub_t=tail.ge(0.5).int()\n",
        "        obj_h=obj_h.ge(0.5).int()\n",
        "        obj_t=obj_t.ge(0.5).int()\n",
        "        sub_hacc += (sub.ge(0.5).int() == sub_htar).sum().item()\n",
        "        sub_tacc += (tail.ge(0.5).int() == sub_ttar).sum().item()\n",
        "        obj_hacc += (obj_h.ge(0.5).int() == obj_htar).sum().item()\n",
        "        obj_tacc += (obj_t.ge(0.5).int() == obj_ttar).sum().item()\n",
        "        train_loss += total_loss.item()\n",
        "        # print('sub_h', sub_h)\n",
        "        # print('sub_t',sub_t)\n",
        "        # print('obj_h',obj_h)\n",
        "        # print('obj_t',obj_t)\n",
        "        # print(sub_hacc)\n",
        "        # print(sub_tacc)\n",
        "        # print(obj_hacc)\n",
        "        # print(obj_tacc)\n",
        "        sub, tail, obj_h, obj_t=sub.cpu(),tail.cpu(),obj_h.cpu(),obj_t.cpu()\n",
        "        sub_heads_logits,sub_tails_logits,obj_heads_logits,obj_tails_logits=np.array(sub),np.array(tail),np.array(obj_h),np.array(obj_t)\n",
        "        inputs_id=inputs_id.cpu()\n",
        "        def extrac_triple(sub_heads_logits,sub_tails_logits,obj_heads_logits,obj_tails_logits,input_ids):  # 抽取一个triple\n",
        "            h_bar=0.5\n",
        "            t_bar=0.5\n",
        "            sub_heads_logits = np.array(sub_heads_logits)\n",
        "            sub_tails_logits = np.array(sub_tails_logits)\n",
        "            tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "            sub_heads, sub_tails = np.where(sub_heads_logits > h_bar)[0], np.where(sub_tails_logits > t_bar)[0]\n",
        "            subjects = []\n",
        "            for sub_head in sub_heads:\n",
        "                sub_tail = sub_tails[sub_tails >= sub_head]\n",
        "                if len(sub_tail) > 0:\n",
        "                    sub_tail = sub_tail[0]\n",
        "                    if sub_tail==sub_head:\n",
        "                        subject = tokens[sub_head]\n",
        "                        subjects.append((subject, sub_head, sub_tail))\n",
        "                    else:\n",
        "                        subject = tokens[sub_head: sub_tail]\n",
        "                        subjects.append((subject, sub_head, sub_tail))\n",
        "            if subjects:\n",
        "                triple_list = []\n",
        "                sub_heads, sub_tails = np.array([sub[1:] for sub in subjects]).T.reshape((2, -1, 1))\n",
        "                for i, subject in enumerate(subjects):\n",
        "                    sub = subject[0]\n",
        "                    sub = ''.join([i.lstrip(\"##\") for i in sub])\n",
        "                    sub = ' '.join(sub.split('[unused1]'))\n",
        "                    obj_heads, obj_tails = np.where(obj_heads_logits > h_bar), np.where(obj_tails_logits > t_bar)\n",
        "                    for obj_head, rel_head in zip(*obj_heads):\n",
        "                        for obj_tail, rel_tail in zip(*obj_tails):\n",
        "                            if obj_head <= obj_tail and rel_head == rel_tail:\n",
        "                                rel = id2rel[rel_head]\n",
        "                                if obj_head==obj_tail:\n",
        "                                    obj=tokens[obj_head]\n",
        "                                else:\n",
        "                                    obj = tokens[obj_head: obj_tail]\n",
        "                                obj = ''.join([i.lstrip(\"##\") for i in obj])\n",
        "                                obj = ' '.join(obj.split('[unused1]'))\n",
        "                                triple_list.append((sub, rel, obj))\n",
        "                                break\n",
        "                triple_set = set()\n",
        "                for s, r, o in triple_list:\n",
        "                    triple_set.add((s, r, o))\n",
        "                return list(triple_set)\n",
        "            else:\n",
        "                return []\n",
        "        for i in range(batch_size):\n",
        "            sub_heads_logits, sub_tails_logits, obj_heads_logits, obj_tails_logits=sub_heads_logits[i],sub_tails_logits[i],obj_heads_logits[i],obj_tails_logits[i]\n",
        "            input_ids=inputs_id[i]\n",
        "            triple_list=extrac_triple(sub_heads_logits,sub_tails_logits,obj_heads_logits,obj_tails_logits,input_ids)\n",
        "            dict_t[j*batch_size+i]=triple_list\n",
        "    # print(dict_t)\n",
        "    json.dump(dict_t,f)\n",
        "    f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xm8UpZWaPXoN"
      },
      "outputs": [],
      "source": [
        "precision, recall, f1_score=metric(test_data,exact_match=False, output_path='triple_list.json')\n",
        "print(precision, recall, f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNRAhPJAK-nv"
      },
      "outputs": [],
      "source": [
        "f=json.load(open('triple_list.json',encoding='utf_8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XPO4KT6LhOI"
      },
      "outputs": [],
      "source": [
        "def load_data(test_path, rel_dict_path):\n",
        "    test_data = json.load(open(test_path))\n",
        "    id2rel, rel2id = json.load(open(rel_dict_path))\n",
        "\n",
        "    id2rel = {int(i): j for i, j in id2rel.items()}\n",
        "    num_rels = len(id2rel)\n",
        "\n",
        "    for sent in test_data:\n",
        "        to_tuple(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9kbBqquK3PD"
      },
      "outputs": [],
      "source": [
        "for i,line in enumerate(test_data):\n",
        "    if i<5:\n",
        "      print(json.loads(line))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "E2EM_NYT_11.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP800/ybnQKkA82lyGHBdgn",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "09415d26246e436f8b1a5db09477fe8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a30bea0eb51a49aab83fba7cf62611af",
              "IPY_MODEL_e064e27eeb674f20816a52739150ce3d",
              "IPY_MODEL_6a675ac2d08f4724a9a78c8a9d9ae8bf"
            ],
            "layout": "IPY_MODEL_119edc1656ef4d2597ec96b4630ca7c6"
          }
        },
        "a30bea0eb51a49aab83fba7cf62611af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c77e2e5a3bf4f50a80b04bcef3f5505",
            "placeholder": "​",
            "style": "IPY_MODEL_e45a0b21d4104f348ea3b0a08e7288ef",
            "value": "Downloading: 100%"
          }
        },
        "e064e27eeb674f20816a52739150ce3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd737be345fd47e6bd2522f5c68cb704",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2b8a2044e4a46479ad82c3180b3ec15",
            "value": 213450
          }
        },
        "6a675ac2d08f4724a9a78c8a9d9ae8bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb82c6df27054a7d917d14205addb6cd",
            "placeholder": "​",
            "style": "IPY_MODEL_b614ac2c203a42f1a595108f36b5d4e4",
            "value": " 208k/208k [00:00&lt;00:00, 2.21MB/s]"
          }
        },
        "119edc1656ef4d2597ec96b4630ca7c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c77e2e5a3bf4f50a80b04bcef3f5505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e45a0b21d4104f348ea3b0a08e7288ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd737be345fd47e6bd2522f5c68cb704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2b8a2044e4a46479ad82c3180b3ec15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb82c6df27054a7d917d14205addb6cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b614ac2c203a42f1a595108f36b5d4e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa8a3f0d6e01477ba77a1a234588370c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbdaa1a798e9428b899f2e50d298d0aa",
              "IPY_MODEL_36f2a713972a48b5a6f85f62bcb50ffe",
              "IPY_MODEL_2cb84df0e6b943e5aae371884179406a"
            ],
            "layout": "IPY_MODEL_eb95445520c747169a25781674a0b512"
          }
        },
        "dbdaa1a798e9428b899f2e50d298d0aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_776e34674d8143638a7b6209933d6a17",
            "placeholder": "​",
            "style": "IPY_MODEL_c7c373ae955f431c8d0f6840348a31c4",
            "value": "Downloading: 100%"
          }
        },
        "36f2a713972a48b5a6f85f62bcb50ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1dea582dcaa43579871f914637bb743",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1584bf3417e343e9abdde2334b07bccb",
            "value": 29
          }
        },
        "2cb84df0e6b943e5aae371884179406a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_025ff577ef554b549de7429d96b2159a",
            "placeholder": "​",
            "style": "IPY_MODEL_590c89d14af24a659c7ec989551933ad",
            "value": " 29.0/29.0 [00:00&lt;00:00, 935B/s]"
          }
        },
        "eb95445520c747169a25781674a0b512": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "776e34674d8143638a7b6209933d6a17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7c373ae955f431c8d0f6840348a31c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1dea582dcaa43579871f914637bb743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1584bf3417e343e9abdde2334b07bccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "025ff577ef554b549de7429d96b2159a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "590c89d14af24a659c7ec989551933ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acced878c7394853a4fbaf356f869854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8fce8b73cdb43658b45e6df6d2afe07",
              "IPY_MODEL_c7a4ec0c9ddc420897b5192be20802ed",
              "IPY_MODEL_8090e57f7fb144839d05432b72460c21"
            ],
            "layout": "IPY_MODEL_0ddae0562a53494295a708cb45dad70a"
          }
        },
        "f8fce8b73cdb43658b45e6df6d2afe07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f690c4f1618b492db34414625189fb65",
            "placeholder": "​",
            "style": "IPY_MODEL_a0dd260faf864d8ab9b20aa17c2f04b1",
            "value": "Downloading: 100%"
          }
        },
        "c7a4ec0c9ddc420897b5192be20802ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9fa3f7abba048869463c5cbf9137afa",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bcf37ece81e4771941724390c6427a1",
            "value": 570
          }
        },
        "8090e57f7fb144839d05432b72460c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9695a18c64d4d7d8b249d697693c505",
            "placeholder": "​",
            "style": "IPY_MODEL_9a93bc8471f346a8b01fa4ec6a619e97",
            "value": " 570/570 [00:00&lt;00:00, 18.4kB/s]"
          }
        },
        "0ddae0562a53494295a708cb45dad70a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f690c4f1618b492db34414625189fb65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0dd260faf864d8ab9b20aa17c2f04b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9fa3f7abba048869463c5cbf9137afa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bcf37ece81e4771941724390c6427a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9695a18c64d4d7d8b249d697693c505": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a93bc8471f346a8b01fa4ec6a619e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72f577a67792411ab0b0b1339a61692a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0fb66c5b719424b9f7a5839542a88df",
              "IPY_MODEL_17a37f9c8f5746429b8d25d7b039045d",
              "IPY_MODEL_710403b1e54248079911cf2a7e6e3647"
            ],
            "layout": "IPY_MODEL_c0339597596c4d42bc39dd15607b1878"
          }
        },
        "b0fb66c5b719424b9f7a5839542a88df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3835ac47fbb4fe6b03dcf8c4d2dce75",
            "placeholder": "​",
            "style": "IPY_MODEL_992624383aef4454ab84938367a7c8d6",
            "value": "Downloading: 100%"
          }
        },
        "17a37f9c8f5746429b8d25d7b039045d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6392d2cae7304c7f922630a619cc397a",
            "max": 435779157,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2caab8d9c236430391dbff71b4af45fa",
            "value": 435779157
          }
        },
        "710403b1e54248079911cf2a7e6e3647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_427f56242a4043599122f7552298c142",
            "placeholder": "​",
            "style": "IPY_MODEL_873821f35d464ba89e2f5ee8fabc862c",
            "value": " 416M/416M [00:07&lt;00:00, 56.8MB/s]"
          }
        },
        "c0339597596c4d42bc39dd15607b1878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3835ac47fbb4fe6b03dcf8c4d2dce75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "992624383aef4454ab84938367a7c8d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6392d2cae7304c7f922630a619cc397a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2caab8d9c236430391dbff71b4af45fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "427f56242a4043599122f7552298c142": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "873821f35d464ba89e2f5ee8fabc862c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}